{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMxHzzfkZ4p4",
        "outputId": "3b0eb0bc-0569-43cc-bb97-b188d8273d25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive,output\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "!ln -s \"/content/drive/MyDrive/proyecto/Codigos\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxBMsBlAZ4p8",
        "outputId": "08a89b65-9e81-4a79-83b8-dae07e346e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content\n",
            "Current working directory: /content/drive/MyDrive/proyecto/Codigos\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Current working directory: {0}\".format(os.getcwd()))\n",
        "# os.chdir(\"/content/drive/MyDrive/Codigos\")\n",
        "os.chdir(\"./Codigos\")\n",
        "# Print the current working directory\n",
        "print(\"Current working directory: {0}\".format(os.getcwd()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBQSfgAGZ4p8",
        "outputId": "3206e2b1-277d-43e1-be41-43b1eb84cada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google.colab in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: ipykernel~=5.3.4 in /usr/local/lib/python3.7/dist-packages (from google.colab) (5.3.4)\n",
            "Requirement already satisfied: google-auth>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from google.colab) (1.35.0)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (1.3.5)\n",
            "Requirement already satisfied: tornado~=5.1.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (5.1.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (2.23.0)\n",
            "Requirement already satisfied: notebook~=5.5.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (5.5.0)\n",
            "Requirement already satisfied: portpicker~=1.3.1 in /usr/local/lib/python3.7/dist-packages (from google.colab) (1.3.9)\n",
            "Requirement already satisfied: astor~=0.8.1 in /usr/local/lib/python3.7/dist-packages (from google.colab) (0.8.1)\n",
            "Requirement already satisfied: ipython~=7.9.0 in /usr/local/lib/python3.7/dist-packages (from google.colab) (7.9.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (0.2.8)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.17.2->google.colab) (4.2.4)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel~=5.3.4->google.colab) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel~=5.3.4->google.colab) (6.1.12)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (2.0.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (4.4.2)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 4.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython~=7.9.0->google.colab) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython~=7.9.0->google.colab) (0.8.3)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (23.2.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (0.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (1.8.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (5.7.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (0.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (2.11.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook~=5.5.0->google.colab) (4.11.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel~=5.3.4->google.colab) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->google.colab) (2022.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->google.colab) (1.21.6)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython~=7.9.0->google.colab) (0.2.5)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.17.2->google.colab) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->google.colab) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->google.colab) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->google.colab) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->google.colab) (2.10)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook~=5.5.0->google.colab) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook~=5.5.0->google.colab) (2.0.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (5.0.1)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook~=5.5.0->google.colab) (0.4)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook~=5.5.0->google.colab) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook~=5.5.0->google.colab) (2.16.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook~=5.5.0->google.colab) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook~=5.5.0->google.colab) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->nbformat->notebook~=5.5.0->google.colab) (3.9.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook~=5.5.0->google.colab) (22.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook~=5.5.0->google.colab) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->notebook~=5.5.0->google.colab) (5.10.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook~=5.5.0->google.colab) (0.5.1)\n",
            "Installing collected packages: jedi\n",
            "Successfully installed jedi-0.18.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting zipfile_deflate64\n",
            "  Downloading zipfile_deflate64-0.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: zipfile-deflate64\n",
            "Successfully installed zipfile-deflate64-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ]
        }
      ],
      "source": [
        "# descarga, descomprimir archivos y lectura pkl\n",
        "!pip install google.colab\n",
        "from google.colab import drive\n",
        "!pip install zipfile_deflate64\n",
        "import zipfile_deflate64 as zipfile\n",
        "import pickle\n",
        "\n",
        "# librerias genericas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# librerias de arquitectura\n",
        "import time\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fPjaereiZ4p9"
      },
      "outputs": [],
      "source": [
        "with open('128.npy', 'rb') as f:\n",
        "    vector_train128 = np.load(f)\n",
        "    labels_train = np.load(f)\n",
        "    vector_val128= np.load(f)\n",
        "    labels_val= np.load(f)\n",
        "    vector_test128= np.load(f)\n",
        "    labels_test= np.load(f)\n",
        "\n",
        "\n",
        "with open('64.npy', 'rb') as f:\n",
        "    vector_train64 = np.load(f)\n",
        "    labels_train = np.load(f)\n",
        "    vector_val64= np.load(f)\n",
        "    labels_val =np.load(f)\n",
        "    vector_test64= np.load(f)\n",
        "    labels_test= np.load(f)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datatrain128 = TensorDataset(torch.Tensor(vector_train128),torch.Tensor(labels_train))\n",
        "dataval128 = TensorDataset(torch.Tensor(vector_val128),torch.Tensor(labels_val))\n",
        "datatest128 = TensorDataset(torch.Tensor(vector_test128),torch.Tensor(labels_test))\n"
      ],
      "metadata": {
        "id": "xefSDMNobyzZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel128(nn.Module):\n",
        "    def __init__(self, dropout_p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 100),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 5),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # result = torch.argmax(, axis=1)\n",
        "        # result = result.reshape(result.shape[0],1)\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "f_1p_jZydF11"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_curves(curves):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(13, 5))\n",
        "    fig.set_facecolor('white')\n",
        "\n",
        "    epochs = np.arange(len(curves[\"val_loss\"])) + 1\n",
        "\n",
        "    ax[0].plot(epochs, curves['val_loss'], label='validation')\n",
        "    ax[0].plot(epochs, curves['train_loss'], label='training')\n",
        "    ax[0].set_xlabel('Epoch')\n",
        "    ax[0].set_ylabel('Loss')\n",
        "    ax[0].set_title('Loss evolution during training')\n",
        "    ax[0].legend()\n",
        "\n",
        "    ax[1].plot(epochs, curves['val_acc'], label='validation')\n",
        "    ax[1].plot(epochs, curves['train_acc'], label='training')\n",
        "    ax[1].set_xlabel('Epoch')\n",
        "    ax[1].set_ylabel('Accuracy')\n",
        "    ax[1].set_title('Accuracy evolution during training')\n",
        "    ax[1].legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ud94CFmB0UIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\"if torch.cuda.is_available() else \"cpu\")\n",
        "model =MLPModel128(dropout_p=0.5).to(device)\n",
        "criterion =nn.CrossEntropyLoss()\n",
        "learning_rate=0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "n_total_steps= len(datatrain128)\n",
        "num_epochs=100\n",
        "batch_size=64\n",
        "use_gpu=True\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(datatrain128, batch_size=batch_size, shuffle=True, pin_memory=use_gpu)\n",
        "val_loader = torch.utils.data.DataLoader(dataval128, batch_size=len(dataval128), shuffle=False, pin_memory=use_gpu)\n",
        "iteration = 0\n",
        "n_batches = len(train_loader)\n",
        "\n",
        "n_evaluations_per_epoch=5\n",
        "curves = {\n",
        "        \"train_acc\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "    }\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\rEpoch {epoch + 1}/{num_epochs}\")\n",
        "    cumulative_train_loss = 0\n",
        "    cumulative_train_corrects = 0\n",
        "    train_loss_count = 0\n",
        "    train_acc_count = 0\n",
        "    for i,(x_batch, y_batch) in enumerate(train_loader):\n",
        "        y_batch = y_batch.type(torch.LongTensor)\n",
        "        if use_gpu:\n",
        "            x_batch = x_batch.cuda()\n",
        "            y_batch = y_batch.cuda()\n",
        "\n",
        "        # Predicción\n",
        "        y_predicted = model(x_batch)\n",
        "\n",
        "        loss = criterion(y_predicted, y_batch)\n",
        "\n",
        "        class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cumulative_train_loss += loss.item()\n",
        "        train_loss_count += 1\n",
        "        train_acc_count += y_batch.shape[0]\n",
        "        cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
        "        if (i % (n_batches // n_evaluations_per_epoch) == 0) and (i > 0):\n",
        "            train_loss = cumulative_train_loss / train_loss_count\n",
        "            train_acc = cumulative_train_corrects / train_acc_count\n",
        "\n",
        "            print(f\"Iteration {iteration} - Batch {i}/{len(train_loader)} - Train loss: {train_loss}, Train acc: {train_acc}\")\n",
        "\n",
        "            iteration += 1\n",
        "    with torch.no_grad():\n",
        "        cumulative_loss = 0\n",
        "        cumulative_predictions = 0\n",
        "        data_count = 0\n",
        "\n",
        "        for x_val, y_val in val_loader:\n",
        "            y_val = y_val.type(torch.LongTensor)\n",
        "            if use_gpu:\n",
        "                x_val = x_val.cuda()\n",
        "                y_val = y_val.cuda()\n",
        "\n",
        "            y_predicted = model(x_val)\n",
        "            \n",
        "            loss = criterion(y_predicted, y_val)\n",
        "\n",
        "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
        "\n",
        "            cumulative_predictions += (y_val == class_prediction).sum().item()\n",
        "            cumulative_loss += loss.item()\n",
        "            data_count += y_val.shape[0]\n",
        "\n",
        "        val_acc = cumulative_predictions / data_count\n",
        "        val_loss = cumulative_loss / len(val_loader)\n",
        "    print(f\"Val loss: {val_loss}, Val acc: {val_acc}\")\n",
        "    curves[\"train_acc\"].append(train_acc)\n",
        "    curves[\"val_acc\"].append(val_acc)\n",
        "    curves[\"train_loss\"].append(train_loss)\n",
        "    curves[\"val_loss\"].append(val_loss)\n",
        "\n",
        "\n",
        "    \n",
        "show_curves(curves)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Htnev6zsdg1T",
        "outputId": "d52919f0-8f5d-483d-cbf9-dde84e9ef1fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpoch 1/100\n",
            "Iteration 0 - Batch 227/1137 - Train loss: 1.620304783185323, Train acc: 0.26973684210526316\n",
            "Iteration 1 - Batch 454/1137 - Train loss: 1.5122697578681694, Train acc: 0.3432348901098901\n",
            "Iteration 2 - Batch 681/1137 - Train loss: 1.385776840521793, Train acc: 0.43101631231671556\n",
            "Iteration 3 - Batch 908/1137 - Train loss: 1.285956607793424, Train acc: 0.48827695269526955\n",
            "Iteration 4 - Batch 1135/1137 - Train loss: 1.2125505875323859, Train acc: 0.5257895026408451\n",
            "Val loss: 0.9072030186653137, Val acc: 0.668\n",
            "Epoch 2/100\n",
            "Iteration 5 - Batch 227/1137 - Train loss: 0.8653362500563002, Train acc: 0.6927768640350878\n",
            "Iteration 6 - Batch 454/1137 - Train loss: 0.8355986031857165, Train acc: 0.7041552197802198\n",
            "Iteration 7 - Batch 681/1137 - Train loss: 0.8198236643918448, Train acc: 0.7082569648093842\n",
            "Iteration 8 - Batch 908/1137 - Train loss: 0.8050505979882084, Train acc: 0.7131463146314632\n",
            "Iteration 9 - Batch 1135/1137 - Train loss: 0.7949385334488371, Train acc: 0.7161366637323944\n",
            "Val loss: 0.7789151668548584, Val acc: 0.736\n",
            "Epoch 3/100\n",
            "Iteration 10 - Batch 227/1137 - Train loss: 0.7153142546875435, Train acc: 0.7455455043859649\n",
            "Iteration 11 - Batch 454/1137 - Train loss: 0.7104366256640507, Train acc: 0.7447458791208791\n",
            "Iteration 12 - Batch 681/1137 - Train loss: 0.7099256104737782, Train acc: 0.7436079545454546\n",
            "Iteration 13 - Batch 908/1137 - Train loss: 0.7045140242484799, Train acc: 0.7455651815181518\n",
            "Iteration 14 - Batch 1135/1137 - Train loss: 0.698339551949585, Train acc: 0.7472491197183099\n",
            "Val loss: 0.7042709589004517, Val acc: 0.752\n",
            "Epoch 4/100\n",
            "Iteration 15 - Batch 227/1137 - Train loss: 0.6561820329281322, Train acc: 0.7639117324561403\n",
            "Iteration 16 - Batch 454/1137 - Train loss: 0.6527898937791259, Train acc: 0.7646291208791208\n",
            "Iteration 17 - Batch 681/1137 - Train loss: 0.649308035450597, Train acc: 0.7642961876832844\n",
            "Iteration 18 - Batch 908/1137 - Train loss: 0.649356187406284, Train acc: 0.764026402640264\n",
            "Iteration 19 - Batch 1135/1137 - Train loss: 0.6465364503608623, Train acc: 0.7653224031690141\n",
            "Val loss: 0.6886934041976929, Val acc: 0.758\n",
            "Epoch 5/100\n",
            "Iteration 20 - Batch 227/1137 - Train loss: 0.6232494473980185, Train acc: 0.7726836622807017\n",
            "Iteration 21 - Batch 454/1137 - Train loss: 0.6230219171597408, Train acc: 0.7734203296703297\n",
            "Iteration 22 - Batch 681/1137 - Train loss: 0.6177874887007073, Train acc: 0.7753848973607038\n",
            "Iteration 23 - Batch 908/1137 - Train loss: 0.6134926011853485, Train acc: 0.7771761551155115\n",
            "Iteration 24 - Batch 1135/1137 - Train loss: 0.611954561001818, Train acc: 0.7774125220070423\n",
            "Val loss: 0.6736841201782227, Val acc: 0.76\n",
            "Epoch 6/100\n",
            "Iteration 25 - Batch 227/1137 - Train loss: 0.6087464698051152, Train acc: 0.7759731359649122\n",
            "Iteration 26 - Batch 454/1137 - Train loss: 0.6023061669789828, Train acc: 0.779739010989011\n",
            "Iteration 27 - Batch 681/1137 - Train loss: 0.5995655242036865, Train acc: 0.7809521627565983\n",
            "Iteration 28 - Batch 908/1137 - Train loss: 0.5960136699532256, Train acc: 0.7820750825082509\n",
            "Iteration 29 - Batch 1135/1137 - Train loss: 0.5904297663135009, Train acc: 0.7849224251760564\n",
            "Val loss: 0.6623488068580627, Val acc: 0.76\n",
            "Epoch 7/100\n",
            "Iteration 30 - Batch 227/1137 - Train loss: 0.5695262216685111, Train acc: 0.7949561403508771\n",
            "Iteration 31 - Batch 454/1137 - Train loss: 0.5768822232445517, Train acc: 0.7916552197802198\n",
            "Iteration 32 - Batch 681/1137 - Train loss: 0.5774834726737741, Train acc: 0.7900247434017595\n",
            "Iteration 33 - Batch 908/1137 - Train loss: 0.5756903429760529, Train acc: 0.7912197469746974\n",
            "Iteration 34 - Batch 1135/1137 - Train loss: 0.572991342323137, Train acc: 0.791868397887324\n",
            "Val loss: 0.6222581267356873, Val acc: 0.778\n",
            "Epoch 8/100\n",
            "Iteration 35 - Batch 227/1137 - Train loss: 0.55178030162004, Train acc: 0.7957785087719298\n",
            "Iteration 36 - Batch 454/1137 - Train loss: 0.5576925354344504, Train acc: 0.7948832417582418\n",
            "Iteration 37 - Batch 681/1137 - Train loss: 0.5582560586614692, Train acc: 0.7964626099706745\n",
            "Iteration 38 - Batch 908/1137 - Train loss: 0.5542539329615364, Train acc: 0.7986626787678768\n",
            "Iteration 39 - Batch 1135/1137 - Train loss: 0.5565845842250217, Train acc: 0.7978102992957746\n",
            "Val loss: 0.630570650100708, Val acc: 0.77\n",
            "Epoch 9/100\n",
            "Iteration 40 - Batch 227/1137 - Train loss: 0.5431394575719248, Train acc: 0.8071546052631579\n",
            "Iteration 41 - Batch 454/1137 - Train loss: 0.5481015705145322, Train acc: 0.804429945054945\n",
            "Iteration 42 - Batch 681/1137 - Train loss: 0.5464511751866411, Train acc: 0.8039314516129032\n",
            "Iteration 43 - Batch 908/1137 - Train loss: 0.545219268211306, Train acc: 0.8035272277227723\n",
            "Iteration 44 - Batch 1135/1137 - Train loss: 0.5426092541081385, Train acc: 0.8048525528169014\n",
            "Val loss: 0.6277623176574707, Val acc: 0.774\n",
            "Epoch 10/100\n",
            "Iteration 45 - Batch 227/1137 - Train loss: 0.5325469195581319, Train acc: 0.8062637061403509\n",
            "Iteration 46 - Batch 454/1137 - Train loss: 0.5312687683891464, Train acc: 0.8101648351648352\n",
            "Iteration 47 - Batch 681/1137 - Train loss: 0.5352259367005217, Train acc: 0.8083990102639296\n",
            "Iteration 48 - Batch 908/1137 - Train loss: 0.5320540065288019, Train acc: 0.8093371837183718\n",
            "Iteration 49 - Batch 1135/1137 - Train loss: 0.5322994128866515, Train acc: 0.8089926276408451\n",
            "Val loss: 0.6037086248397827, Val acc: 0.77\n",
            "Epoch 11/100\n",
            "Iteration 50 - Batch 227/1137 - Train loss: 0.5194637842084232, Train acc: 0.8143503289473685\n",
            "Iteration 51 - Batch 454/1137 - Train loss: 0.5204714642776238, Train acc: 0.8140796703296703\n",
            "Iteration 52 - Batch 681/1137 - Train loss: 0.5242656183120442, Train acc: 0.8116064882697948\n",
            "Iteration 53 - Batch 908/1137 - Train loss: 0.5199612039576794, Train acc: 0.8125859460946094\n",
            "Iteration 54 - Batch 1135/1137 - Train loss: 0.5198751341315432, Train acc: 0.812073613556338\n",
            "Val loss: 0.6056951284408569, Val acc: 0.784\n",
            "Epoch 12/100\n",
            "Iteration 55 - Batch 227/1137 - Train loss: 0.5028656962932202, Train acc: 0.8194901315789473\n",
            "Iteration 56 - Batch 454/1137 - Train loss: 0.504045751952863, Train acc: 0.8199175824175824\n",
            "Iteration 57 - Batch 681/1137 - Train loss: 0.5118326712782082, Train acc: 0.8168988269794721\n",
            "Iteration 58 - Batch 908/1137 - Train loss: 0.5119457737480191, Train acc: 0.8172442244224423\n",
            "Iteration 59 - Batch 1135/1137 - Train loss: 0.5127349420463745, Train acc: 0.8164887764084507\n",
            "Val loss: 0.6115590929985046, Val acc: 0.772\n",
            "Epoch 13/100\n",
            "Iteration 60 - Batch 227/1137 - Train loss: 0.5027582248051962, Train acc: 0.8206551535087719\n",
            "Iteration 61 - Batch 454/1137 - Train loss: 0.5055599969166976, Train acc: 0.8200892857142857\n",
            "Iteration 62 - Batch 681/1137 - Train loss: 0.5060187246093303, Train acc: 0.8198313782991202\n",
            "Iteration 63 - Batch 908/1137 - Train loss: 0.5049539096612479, Train acc: 0.8188600110011001\n",
            "Iteration 64 - Batch 1135/1137 - Train loss: 0.5033148997135355, Train acc: 0.8198035871478874\n",
            "Val loss: 0.5914453268051147, Val acc: 0.784\n",
            "Epoch 14/100\n",
            "Iteration 65 - Batch 227/1137 - Train loss: 0.49741129067383316, Train acc: 0.8203810307017544\n",
            "Iteration 66 - Batch 454/1137 - Train loss: 0.49495986559233823, Train acc: 0.8221840659340659\n",
            "Iteration 67 - Batch 681/1137 - Train loss: 0.4929300192470425, Train acc: 0.8228555718475073\n",
            "Iteration 68 - Batch 908/1137 - Train loss: 0.4920199623431834, Train acc: 0.8233120187018702\n",
            "Iteration 69 - Batch 1135/1137 - Train loss: 0.49291729044274124, Train acc: 0.8231046434859155\n",
            "Val loss: 0.5990963578224182, Val acc: 0.778\n",
            "Epoch 15/100\n",
            "Iteration 70 - Batch 227/1137 - Train loss: 0.478486727596375, Train acc: 0.8277823464912281\n",
            "Iteration 71 - Batch 454/1137 - Train loss: 0.48706907498967517, Train acc: 0.8253434065934065\n",
            "Iteration 72 - Batch 681/1137 - Train loss: 0.48431066028183856, Train acc: 0.8260859604105572\n",
            "Iteration 73 - Batch 908/1137 - Train loss: 0.48520767021559513, Train acc: 0.825735698569857\n",
            "Iteration 74 - Batch 1135/1137 - Train loss: 0.4858703396060097, Train acc: 0.8261031029929577\n",
            "Val loss: 0.5598024725914001, Val acc: 0.796\n",
            "Epoch 16/100\n",
            "Iteration 75 - Batch 227/1137 - Train loss: 0.48289763234686434, Train acc: 0.8246299342105263\n",
            "Iteration 76 - Batch 454/1137 - Train loss: 0.480777649630557, Train acc: 0.8252747252747252\n",
            "Iteration 77 - Batch 681/1137 - Train loss: 0.4774986497642707, Train acc: 0.8273460410557185\n",
            "Iteration 78 - Batch 908/1137 - Train loss: 0.47832790512194073, Train acc: 0.8272999174917491\n",
            "Iteration 79 - Batch 1135/1137 - Train loss: 0.4776576607632385, Train acc: 0.8271346830985915\n",
            "Val loss: 0.5947639346122742, Val acc: 0.784\n",
            "Epoch 17/100\n",
            "Iteration 80 - Batch 227/1137 - Train loss: 0.4758110753538316, Train acc: 0.8331277412280702\n",
            "Iteration 81 - Batch 454/1137 - Train loss: 0.478563682784091, Train acc: 0.8317651098901099\n",
            "Iteration 82 - Batch 681/1137 - Train loss: 0.47301952921050733, Train acc: 0.8328445747800587\n",
            "Iteration 83 - Batch 908/1137 - Train loss: 0.47071281788569475, Train acc: 0.832903602860286\n",
            "Iteration 84 - Batch 1135/1137 - Train loss: 0.4713336024826891, Train acc: 0.832732724471831\n",
            "Val loss: 0.5939481854438782, Val acc: 0.78\n",
            "Epoch 18/100\n",
            "Iteration 85 - Batch 227/1137 - Train loss: 0.46501697527996283, Train acc: 0.8315515350877193\n",
            "Iteration 86 - Batch 454/1137 - Train loss: 0.4590922676927441, Train acc: 0.8355425824175824\n",
            "Iteration 87 - Batch 681/1137 - Train loss: 0.46141305543984834, Train acc: 0.8340359237536656\n",
            "Iteration 88 - Batch 908/1137 - Train loss: 0.4634401206300072, Train acc: 0.8336083608360836\n",
            "Iteration 89 - Batch 1135/1137 - Train loss: 0.4633283205986233, Train acc: 0.8338193221830986\n",
            "Val loss: 0.5986633896827698, Val acc: 0.792\n",
            "Epoch 19/100\n",
            "Iteration 90 - Batch 227/1137 - Train loss: 0.45338064870029166, Train acc: 0.8414884868421053\n",
            "Iteration 91 - Batch 454/1137 - Train loss: 0.4537987985781261, Train acc: 0.8416552197802197\n",
            "Iteration 92 - Batch 681/1137 - Train loss: 0.45425658343124947, Train acc: 0.8405196114369502\n",
            "Iteration 93 - Batch 908/1137 - Train loss: 0.4548284278975593, Train acc: 0.8392464246424642\n",
            "Iteration 94 - Batch 1135/1137 - Train loss: 0.4569686599941531, Train acc: 0.8382619938380281\n",
            "Val loss: 0.6010119915008545, Val acc: 0.778\n",
            "Epoch 20/100\n",
            "Iteration 95 - Batch 227/1137 - Train loss: 0.45485088235714977, Train acc: 0.8391584429824561\n",
            "Iteration 96 - Batch 454/1137 - Train loss: 0.4498530646929374, Train acc: 0.8400755494505494\n",
            "Iteration 97 - Batch 681/1137 - Train loss: 0.44981985394993135, Train acc: 0.8398093841642229\n",
            "Iteration 98 - Batch 908/1137 - Train loss: 0.44951053996487417, Train acc: 0.8401402640264026\n",
            "Iteration 99 - Batch 1135/1137 - Train loss: 0.4513416483893361, Train acc: 0.8396374339788732\n",
            "Val loss: 0.6261683702468872, Val acc: 0.78\n",
            "Epoch 21/100\n",
            "Iteration 100 - Batch 227/1137 - Train loss: 0.44251570144766256, Train acc: 0.8416940789473685\n",
            "Iteration 101 - Batch 454/1137 - Train loss: 0.4423100567126012, Train acc: 0.8427541208791208\n",
            "Iteration 102 - Batch 681/1137 - Train loss: 0.44274649790916976, Train acc: 0.8415964076246334\n",
            "Iteration 103 - Batch 908/1137 - Train loss: 0.4419409611863796, Train acc: 0.8423920517051705\n",
            "Iteration 104 - Batch 1135/1137 - Train loss: 0.4430644402647732, Train acc: 0.8415630501760564\n",
            "Val loss: 0.6059742569923401, Val acc: 0.794\n",
            "Epoch 22/100\n",
            "Iteration 105 - Batch 227/1137 - Train loss: 0.43488192617108945, Train acc: 0.8469709429824561\n",
            "Iteration 106 - Batch 454/1137 - Train loss: 0.43538138064054344, Train acc: 0.8459134615384616\n",
            "Iteration 107 - Batch 681/1137 - Train loss: 0.43693286031921585, Train acc: 0.8454682917888563\n",
            "Iteration 108 - Batch 908/1137 - Train loss: 0.4388126139954241, Train acc: 0.8438359460946094\n",
            "Iteration 109 - Batch 1135/1137 - Train loss: 0.440260545337494, Train acc: 0.8432961047535211\n",
            "Val loss: 0.6072097420692444, Val acc: 0.784\n",
            "Epoch 23/100\n",
            "Iteration 110 - Batch 227/1137 - Train loss: 0.4404863400156038, Train acc: 0.8431332236842105\n",
            "Iteration 111 - Batch 454/1137 - Train loss: 0.4321627939140404, Train acc: 0.8464285714285714\n",
            "Iteration 112 - Batch 681/1137 - Train loss: 0.434662879846138, Train acc: 0.8452620967741935\n",
            "Iteration 113 - Batch 908/1137 - Train loss: 0.43429346417043074, Train acc: 0.8457955170517052\n",
            "Iteration 114 - Batch 1135/1137 - Train loss: 0.43478048133703184, Train acc: 0.845826914612676\n",
            "Val loss: 0.6031343340873718, Val acc: 0.786\n",
            "Epoch 24/100\n",
            "Iteration 115 - Batch 227/1137 - Train loss: 0.43260586947986956, Train acc: 0.8464912280701754\n",
            "Iteration 116 - Batch 454/1137 - Train loss: 0.4297217286550082, Train acc: 0.8476991758241759\n",
            "Iteration 117 - Batch 681/1137 - Train loss: 0.43084811624631153, Train acc: 0.8451933651026393\n",
            "Iteration 118 - Batch 908/1137 - Train loss: 0.4291283569090699, Train acc: 0.8464315181518152\n",
            "Iteration 119 - Batch 1135/1137 - Train loss: 0.429418649865498, Train acc: 0.8466934419014085\n",
            "Val loss: 0.6190436482429504, Val acc: 0.786\n",
            "Epoch 25/100\n",
            "Iteration 120 - Batch 227/1137 - Train loss: 0.4232168383896351, Train acc: 0.8523848684210527\n",
            "Iteration 121 - Batch 454/1137 - Train loss: 0.42238513064253463, Train acc: 0.851510989010989\n",
            "Iteration 122 - Batch 681/1137 - Train loss: 0.42287971796003604, Train acc: 0.850921004398827\n",
            "Iteration 123 - Batch 908/1137 - Train loss: 0.4238932284027567, Train acc: 0.8499209295929593\n",
            "Iteration 124 - Batch 1135/1137 - Train loss: 0.4230457838152496, Train acc: 0.8501320422535211\n",
            "Val loss: 0.6272333264350891, Val acc: 0.788\n",
            "Epoch 26/100\n",
            "Iteration 125 - Batch 227/1137 - Train loss: 0.4109193019307496, Train acc: 0.854235197368421\n",
            "Iteration 126 - Batch 454/1137 - Train loss: 0.41127272739515197, Train acc: 0.8552884615384615\n",
            "Iteration 127 - Batch 681/1137 - Train loss: 0.4176699609831631, Train acc: 0.8521810850439883\n",
            "Iteration 128 - Batch 908/1137 - Train loss: 0.41908464942238116, Train acc: 0.8520008250825083\n",
            "Iteration 129 - Batch 1135/1137 - Train loss: 0.4182648895518251, Train acc: 0.8521539392605634\n",
            "Val loss: 0.6171106696128845, Val acc: 0.782\n",
            "Epoch 27/100\n",
            "Iteration 130 - Batch 227/1137 - Train loss: 0.41918621310277987, Train acc: 0.8525904605263158\n",
            "Iteration 131 - Batch 454/1137 - Train loss: 0.4143803323690708, Train acc: 0.8542582417582417\n",
            "Iteration 132 - Batch 681/1137 - Train loss: 0.4157902603406361, Train acc: 0.8528225806451613\n",
            "Iteration 133 - Batch 908/1137 - Train loss: 0.4164030041598906, Train acc: 0.8527915291529153\n",
            "Iteration 134 - Batch 1135/1137 - Train loss: 0.41418603450899394, Train acc: 0.8538594850352113\n",
            "Val loss: 0.6378722190856934, Val acc: 0.786\n",
            "Epoch 28/100\n",
            "Iteration 135 - Batch 227/1137 - Train loss: 0.4098863118050391, Train acc: 0.8534128289473685\n",
            "Iteration 136 - Batch 454/1137 - Train loss: 0.41118802454445386, Train acc: 0.8540178571428572\n",
            "Iteration 137 - Batch 681/1137 - Train loss: 0.41015093087538246, Train acc: 0.8535557184750733\n",
            "Iteration 138 - Batch 908/1137 - Train loss: 0.4095892816191853, Train acc: 0.853805693069307\n",
            "Iteration 139 - Batch 1135/1137 - Train loss: 0.4100754919763602, Train acc: 0.8535981514084507\n",
            "Val loss: 0.6143115162849426, Val acc: 0.786\n",
            "Epoch 29/100\n",
            "Iteration 140 - Batch 227/1137 - Train loss: 0.4008054475120285, Train acc: 0.8579358552631579\n",
            "Iteration 141 - Batch 454/1137 - Train loss: 0.40107946212475115, Train acc: 0.8568337912087912\n",
            "Iteration 142 - Batch 681/1137 - Train loss: 0.40529996796874357, Train acc: 0.8558238636363636\n",
            "Iteration 143 - Batch 908/1137 - Train loss: 0.40601975102760346, Train acc: 0.8554042904290429\n",
            "Iteration 144 - Batch 1135/1137 - Train loss: 0.4055746823391864, Train acc: 0.8561014524647887\n",
            "Val loss: 0.6372848153114319, Val acc: 0.78\n",
            "Epoch 30/100\n",
            "Iteration 145 - Batch 227/1137 - Train loss: 0.39829694231351215, Train acc: 0.8573190789473685\n",
            "Iteration 146 - Batch 454/1137 - Train loss: 0.39532294224251757, Train acc: 0.858070054945055\n",
            "Iteration 147 - Batch 681/1137 - Train loss: 0.3988458012118717, Train acc: 0.8567402859237536\n",
            "Iteration 148 - Batch 908/1137 - Train loss: 0.4003051467428375, Train acc: 0.8567278602860287\n",
            "Iteration 149 - Batch 1135/1137 - Train loss: 0.4013153010926826, Train acc: 0.8568166813380281\n",
            "Val loss: 0.5843918919563293, Val acc: 0.8\n",
            "Epoch 31/100\n",
            "Iteration 150 - Batch 227/1137 - Train loss: 0.3965737241271295, Train acc: 0.8596491228070176\n",
            "Iteration 151 - Batch 454/1137 - Train loss: 0.3966767342863502, Train acc: 0.8597870879120879\n",
            "Iteration 152 - Batch 681/1137 - Train loss: 0.39551784257647576, Train acc: 0.8591229838709677\n",
            "Iteration 153 - Batch 908/1137 - Train loss: 0.39363691743516555, Train acc: 0.8595297029702971\n",
            "Iteration 154 - Batch 1135/1137 - Train loss: 0.3949033992338768, Train acc: 0.8590586487676056\n",
            "Val loss: 0.6319109201431274, Val acc: 0.796\n",
            "Epoch 32/100\n",
            "Iteration 155 - Batch 227/1137 - Train loss: 0.3892203542616284, Train acc: 0.8608141447368421\n",
            "Iteration 156 - Batch 454/1137 - Train loss: 0.38861058806325055, Train acc: 0.8611607142857143\n",
            "Iteration 157 - Batch 681/1137 - Train loss: 0.3910710058472723, Train acc: 0.8597186583577713\n",
            "Iteration 158 - Batch 908/1137 - Train loss: 0.3904056537269366, Train acc: 0.8604407315731574\n",
            "Iteration 159 - Batch 1135/1137 - Train loss: 0.3923207903543199, Train acc: 0.859375\n",
            "Val loss: 0.631052553653717, Val acc: 0.772\n",
            "Epoch 33/100\n",
            "Iteration 160 - Batch 227/1137 - Train loss: 0.3795977119813886, Train acc: 0.8626644736842105\n",
            "Iteration 161 - Batch 454/1137 - Train loss: 0.38118203095682374, Train acc: 0.8645947802197802\n",
            "Iteration 162 - Batch 681/1137 - Train loss: 0.38598669516987816, Train acc: 0.8621700879765396\n",
            "Iteration 163 - Batch 908/1137 - Train loss: 0.38841712642030746, Train acc: 0.8615580308030804\n",
            "Iteration 164 - Batch 1135/1137 - Train loss: 0.3872111039892049, Train acc: 0.8618645466549296\n",
            "Val loss: 0.6653810143470764, Val acc: 0.782\n",
            "Epoch 34/100\n",
            "Iteration 165 - Batch 227/1137 - Train loss: 0.379461873976285, Train acc: 0.8634183114035088\n",
            "Iteration 166 - Batch 454/1137 - Train loss: 0.38527867672862587, Train acc: 0.8621222527472527\n",
            "Iteration 167 - Batch 681/1137 - Train loss: 0.3860201267771357, Train acc: 0.8623533724340176\n",
            "Iteration 168 - Batch 908/1137 - Train loss: 0.3855275300109085, Train acc: 0.8622284103410341\n",
            "Iteration 169 - Batch 1135/1137 - Train loss: 0.3853227532350681, Train acc: 0.8626485475352113\n",
            "Val loss: 0.6510179042816162, Val acc: 0.786\n",
            "Epoch 35/100\n",
            "Iteration 170 - Batch 227/1137 - Train loss: 0.372074754679935, Train acc: 0.8689692982456141\n",
            "Iteration 171 - Batch 454/1137 - Train loss: 0.37796450437425255, Train acc: 0.8669642857142857\n",
            "Iteration 172 - Batch 681/1137 - Train loss: 0.37818411307251, Train acc: 0.8655379398826979\n",
            "Iteration 173 - Batch 908/1137 - Train loss: 0.37974428850235326, Train acc: 0.8646177117711771\n",
            "Iteration 174 - Batch 1135/1137 - Train loss: 0.3808608497427383, Train acc: 0.8646979533450704\n",
            "Val loss: 0.6925404667854309, Val acc: 0.772\n",
            "Epoch 36/100\n",
            "Iteration 175 - Batch 227/1137 - Train loss: 0.3796246962850554, Train acc: 0.8636239035087719\n",
            "Iteration 176 - Batch 454/1137 - Train loss: 0.3763299630893456, Train acc: 0.866929945054945\n",
            "Iteration 177 - Batch 681/1137 - Train loss: 0.37881075245631407, Train acc: 0.8659503299120235\n",
            "Iteration 178 - Batch 908/1137 - Train loss: 0.3770162512202992, Train acc: 0.8668179317931793\n",
            "Iteration 179 - Batch 1135/1137 - Train loss: 0.3774964638238012, Train acc: 0.8664034991197183\n",
            "Val loss: 0.6278362274169922, Val acc: 0.79\n",
            "Epoch 37/100\n",
            "Iteration 180 - Batch 227/1137 - Train loss: 0.3757086898804757, Train acc: 0.8667763157894737\n",
            "Iteration 181 - Batch 454/1137 - Train loss: 0.37178220044780563, Train acc: 0.8667239010989011\n",
            "Iteration 182 - Batch 681/1137 - Train loss: 0.37233984715078583, Train acc: 0.8676228005865103\n",
            "Iteration 183 - Batch 908/1137 - Train loss: 0.372195202017119, Train acc: 0.8669382563256326\n",
            "Iteration 184 - Batch 1135/1137 - Train loss: 0.3733319214768183, Train acc: 0.8664310079225352\n",
            "Val loss: 0.6890181303024292, Val acc: 0.784\n",
            "Epoch 38/100\n",
            "Iteration 185 - Batch 227/1137 - Train loss: 0.37364256297025766, Train acc: 0.8654057017543859\n",
            "Iteration 186 - Batch 454/1137 - Train loss: 0.37284741211723493, Train acc: 0.867239010989011\n",
            "Iteration 187 - Batch 681/1137 - Train loss: 0.3705527969687915, Train acc: 0.8679435483870968\n",
            "Iteration 188 - Batch 908/1137 - Train loss: 0.3709661811918053, Train acc: 0.8666116611661167\n",
            "Iteration 189 - Batch 1135/1137 - Train loss: 0.37005283957688323, Train acc: 0.8673800616197183\n",
            "Val loss: 0.7096751928329468, Val acc: 0.798\n",
            "Epoch 39/100\n",
            "Iteration 190 - Batch 227/1137 - Train loss: 0.3666712323432429, Train acc: 0.8697916666666666\n",
            "Iteration 191 - Batch 454/1137 - Train loss: 0.36730808324211245, Train acc: 0.8694024725274725\n",
            "Iteration 192 - Batch 681/1137 - Train loss: 0.3683720340520755, Train acc: 0.8689057917888563\n",
            "Iteration 193 - Batch 908/1137 - Train loss: 0.3683369242646644, Train acc: 0.8691728547854786\n",
            "Iteration 194 - Batch 1135/1137 - Train loss: 0.3651344010239126, Train acc: 0.8697458186619719\n",
            "Val loss: 0.6964425444602966, Val acc: 0.794\n",
            "Epoch 40/100\n",
            "Iteration 195 - Batch 227/1137 - Train loss: 0.3596283881026402, Train acc: 0.8697916666666666\n",
            "Iteration 196 - Batch 454/1137 - Train loss: 0.3609802857205108, Train acc: 0.8704326923076923\n",
            "Iteration 197 - Batch 681/1137 - Train loss: 0.3638757792604634, Train acc: 0.8696847507331378\n",
            "Iteration 198 - Batch 908/1137 - Train loss: 0.366213059267982, Train acc: 0.8687431243124313\n",
            "Iteration 199 - Batch 1135/1137 - Train loss: 0.36391962517026655, Train acc: 0.8694707306338029\n",
            "Val loss: 0.7266585826873779, Val acc: 0.782\n",
            "Epoch 41/100\n",
            "Iteration 200 - Batch 227/1137 - Train loss: 0.34980161095920365, Train acc: 0.8767132675438597\n",
            "Iteration 201 - Batch 454/1137 - Train loss: 0.3580925165296911, Train acc: 0.871875\n",
            "Iteration 202 - Batch 681/1137 - Train loss: 0.3613642564267357, Train acc: 0.8708760997067448\n",
            "Iteration 203 - Batch 908/1137 - Train loss: 0.36077822644849555, Train acc: 0.8706339383938394\n",
            "Iteration 204 - Batch 1135/1137 - Train loss: 0.3606986820330502, Train acc: 0.8706261003521126\n",
            "Val loss: 0.7224544882774353, Val acc: 0.774\n",
            "Epoch 42/100\n",
            "Iteration 205 - Batch 227/1137 - Train loss: 0.35058014539250154, Train acc: 0.8752055921052632\n",
            "Iteration 206 - Batch 454/1137 - Train loss: 0.353875455063778, Train acc: 0.8745535714285714\n",
            "Iteration 207 - Batch 681/1137 - Train loss: 0.35280008295064796, Train acc: 0.873625366568915\n",
            "Iteration 208 - Batch 908/1137 - Train loss: 0.35593299369345144, Train acc: 0.8719918866886689\n",
            "Iteration 209 - Batch 1135/1137 - Train loss: 0.3574408051282377, Train acc: 0.871217539612676\n",
            "Val loss: 0.737992525100708, Val acc: 0.772\n",
            "Epoch 43/100\n",
            "Iteration 210 - Batch 227/1137 - Train loss: 0.3523432602615733, Train acc: 0.8726699561403509\n",
            "Iteration 211 - Batch 454/1137 - Train loss: 0.3467222974195585, Train acc: 0.8758585164835165\n",
            "Iteration 212 - Batch 681/1137 - Train loss: 0.3460120539839142, Train acc: 0.875824780058651\n",
            "Iteration 213 - Batch 908/1137 - Train loss: 0.34988788226814027, Train acc: 0.8745015126512651\n",
            "Iteration 214 - Batch 1135/1137 - Train loss: 0.3541857143995208, Train acc: 0.8739959286971831\n",
            "Val loss: 0.7205700874328613, Val acc: 0.792\n",
            "Epoch 44/100\n",
            "Iteration 215 - Batch 227/1137 - Train loss: 0.3643119351513553, Train acc: 0.8712993421052632\n",
            "Iteration 216 - Batch 454/1137 - Train loss: 0.36140839974958816, Train acc: 0.8711538461538462\n",
            "Iteration 217 - Batch 681/1137 - Train loss: 0.3573963701768704, Train acc: 0.8714259530791789\n",
            "Iteration 218 - Batch 908/1137 - Train loss: 0.35306358417727635, Train acc: 0.8730232398239824\n",
            "Iteration 219 - Batch 1135/1137 - Train loss: 0.35261874647260133, Train acc: 0.8739271566901409\n",
            "Val loss: 0.7428079843521118, Val acc: 0.794\n",
            "Epoch 45/100\n",
            "Iteration 220 - Batch 227/1137 - Train loss: 0.3403555633206117, Train acc: 0.8781524122807017\n",
            "Iteration 221 - Batch 454/1137 - Train loss: 0.3449390487683998, Train acc: 0.8765796703296703\n",
            "Iteration 222 - Batch 681/1137 - Train loss: 0.3437233362073773, Train acc: 0.8767641129032258\n",
            "Iteration 223 - Batch 908/1137 - Train loss: 0.3448355465274964, Train acc: 0.8762548129812981\n",
            "Iteration 224 - Batch 1135/1137 - Train loss: 0.34610958720072055, Train acc: 0.8758665272887324\n",
            "Val loss: 0.7392711639404297, Val acc: 0.778\n",
            "Epoch 46/100\n",
            "Iteration 225 - Batch 227/1137 - Train loss: 0.34915387104347084, Train acc: 0.8738349780701754\n",
            "Iteration 226 - Batch 454/1137 - Train loss: 0.3505941376745046, Train acc: 0.8745192307692308\n",
            "Iteration 227 - Batch 681/1137 - Train loss: 0.346870046720739, Train acc: 0.8764891862170088\n",
            "Iteration 228 - Batch 908/1137 - Train loss: 0.34653081056406787, Train acc: 0.8763063806380638\n",
            "Iteration 229 - Batch 1135/1137 - Train loss: 0.3465661397138217, Train acc: 0.8761691241197183\n",
            "Val loss: 0.7537077069282532, Val acc: 0.772\n",
            "Epoch 47/100\n",
            "Iteration 230 - Batch 227/1137 - Train loss: 0.3449664262303135, Train acc: 0.875\n",
            "Iteration 231 - Batch 454/1137 - Train loss: 0.34016981419626174, Train acc: 0.876717032967033\n",
            "Iteration 232 - Batch 681/1137 - Train loss: 0.3416171066703335, Train acc: 0.8763059017595308\n",
            "Iteration 233 - Batch 908/1137 - Train loss: 0.339016590378072, Train acc: 0.8772861661166117\n",
            "Iteration 234 - Batch 1135/1137 - Train loss: 0.3399432657784979, Train acc: 0.8766642825704225\n",
            "Val loss: 0.7423571944236755, Val acc: 0.782\n",
            "Epoch 48/100\n",
            "Iteration 235 - Batch 227/1137 - Train loss: 0.3343739241949822, Train acc: 0.877672697368421\n",
            "Iteration 236 - Batch 454/1137 - Train loss: 0.3365727966467103, Train acc: 0.8764079670329671\n",
            "Iteration 237 - Batch 681/1137 - Train loss: 0.3382372830467315, Train acc: 0.8769473973607038\n",
            "Iteration 238 - Batch 908/1137 - Train loss: 0.3388151819146637, Train acc: 0.8775268151815182\n",
            "Iteration 239 - Batch 1135/1137 - Train loss: 0.34033223062517565, Train acc: 0.8771319322183099\n",
            "Val loss: 0.7422155737876892, Val acc: 0.798\n",
            "Epoch 49/100\n",
            "Iteration 240 - Batch 227/1137 - Train loss: 0.33126682386194406, Train acc: 0.8795915570175439\n",
            "Iteration 241 - Batch 454/1137 - Train loss: 0.3295976753909509, Train acc: 0.881043956043956\n",
            "Iteration 242 - Batch 681/1137 - Train loss: 0.33005859561898143, Train acc: 0.880338159824047\n",
            "Iteration 243 - Batch 908/1137 - Train loss: 0.33304866334491046, Train acc: 0.878833195819582\n",
            "Iteration 244 - Batch 1135/1137 - Train loss: 0.3357057309788193, Train acc: 0.8783835827464789\n",
            "Val loss: 0.788043200969696, Val acc: 0.782\n",
            "Epoch 50/100\n",
            "Iteration 245 - Batch 227/1137 - Train loss: 0.3268753514347369, Train acc: 0.8826069078947368\n",
            "Iteration 246 - Batch 454/1137 - Train loss: 0.33478461166659557, Train acc: 0.8803914835164836\n",
            "Iteration 247 - Batch 681/1137 - Train loss: 0.3333749709503392, Train acc: 0.8801777859237536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nsJsYqEEqBhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datatrain64 = TensorDataset(torch.Tensor(vector_train64),torch.Tensor(labels_train))\n",
        "dataval64 = TensorDataset(torch.Tensor(vector_val64),torch.Tensor(labels_val))\n",
        "datatest64 = TensorDataset(torch.Tensor(vector_test64),torch.Tensor(labels_test))"
      ],
      "metadata": {
        "id": "MOu4iIA-V-KA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel64(nn.Module):\n",
        "    def __init__(self, dropout_p):\n",
        "        super().__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 50),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 35),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(35, 20),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(20, 15),\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(15, 10),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(10, 5),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # result = torch.argmax(, axis=1)\n",
        "        # result = result.reshape(result.shape[0],1)\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "XEZcp9b5f3sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\"if torch.cuda.is_available() else \"cpu\")\n",
        "model =MLPModel64(dropout_p=0.5).to(device)\n",
        "criterion =nn.CrossEntropyLoss()\n",
        "learning_rate=0.0001\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "n_total_steps= len(datatrain64)\n",
        "num_epochs=100\n",
        "batch_size=64\n",
        "use_gpu=True\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(datatrain64, batch_size=batch_size, shuffle=True, pin_memory=use_gpu)\n",
        "val_loader = torch.utils.data.DataLoader(datatest64, batch_size=len(datatest64), shuffle=False, pin_memory=use_gpu)\n",
        "iteration = 0\n",
        "n_batches = len(train_loader)\n",
        "\n",
        "n_evaluations_per_epoch=5\n",
        "curves = {\n",
        "        \"train_acc\": [],\n",
        "        \"val_acc\": [],\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "    }\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\rEpoch {epoch + 1}/{num_epochs}\")\n",
        "    cumulative_train_loss = 0\n",
        "    cumulative_train_corrects = 0\n",
        "    train_loss_count = 0\n",
        "    train_acc_count = 0\n",
        "    for i,(x_batch, y_batch) in enumerate(train_loader):\n",
        "        y_batch = y_batch.type(torch.LongTensor)\n",
        "        if use_gpu:\n",
        "            x_batch = x_batch.cuda()\n",
        "            y_batch = y_batch.cuda()\n",
        "\n",
        "        # Predicción\n",
        "        y_predicted = model(x_batch)\n",
        "\n",
        "        loss = criterion(y_predicted, y_batch)\n",
        "\n",
        "        class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        cumulative_train_loss += loss.item()\n",
        "        train_loss_count += 1\n",
        "        train_acc_count += y_batch.shape[0]\n",
        "        cumulative_train_corrects += (y_batch == class_prediction).sum().item()\n",
        "        if (i % (n_batches // n_evaluations_per_epoch) == 0) and (i > 0):\n",
        "            train_loss = cumulative_train_loss / train_loss_count\n",
        "            train_acc = cumulative_train_corrects / train_acc_count\n",
        "\n",
        "            print(f\"Iteration {iteration} - Batch {i}/{len(train_loader)} - Train loss: {train_loss}, Train acc: {train_acc}\")\n",
        "\n",
        "            iteration += 1\n",
        "    with torch.no_grad():\n",
        "        cumulative_loss = 0\n",
        "        cumulative_predictions = 0\n",
        "        data_count = 0\n",
        "\n",
        "        for x_val, y_val in val_loader:\n",
        "            y_val = y_val.type(torch.LongTensor)\n",
        "            if use_gpu:\n",
        "                x_val = x_val.cuda()\n",
        "                y_val = y_val.cuda()\n",
        "\n",
        "            y_predicted = model(x_val)\n",
        "            \n",
        "            loss = criterion(y_predicted, y_val)\n",
        "\n",
        "            class_prediction = torch.argmax(y_predicted, axis=1).long()\n",
        "\n",
        "            cumulative_predictions += (y_val == class_prediction).sum().item()\n",
        "            cumulative_loss += loss.item()\n",
        "            data_count += y_val.shape[0]\n",
        "\n",
        "        val_acc = cumulative_predictions / data_count\n",
        "        val_loss = cumulative_loss / len(val_loader)\n",
        "    print(f\"Val loss: {val_loss}, Val acc: {val_acc}\")\n",
        "    curves[\"train_acc\"].append(train_acc)\n",
        "    curves[\"val_acc\"].append(val_acc)\n",
        "    curves[\"train_loss\"].append(train_loss)\n",
        "    curves[\"val_loss\"].append(val_loss)\n",
        "\n",
        "\n",
        "    \n",
        "show_curves(curves)"
      ],
      "metadata": {
        "id": "FMPLmSshg7Fo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f792997-0721-4ca5-ba8f-dc18c18e80be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpoch 1/100\n",
            "Iteration 0 - Batch 227/1137 - Train loss: 1.6186585562270985, Train acc: 0.20079495614035087\n",
            "Iteration 1 - Batch 454/1137 - Train loss: 1.5788027540668026, Train acc: 0.22156593406593406\n",
            "Iteration 2 - Batch 681/1137 - Train loss: 1.504291082582166, Train acc: 0.26672470674486803\n",
            "Iteration 3 - Batch 908/1137 - Train loss: 1.4360969094159972, Train acc: 0.29646245874587457\n",
            "Iteration 4 - Batch 1135/1137 - Train loss: 1.3887190053790388, Train acc: 0.3171352332746479\n",
            "Val loss: 1.1654162406921387, Val acc: 0.424\n",
            "Epoch 2/100\n",
            "Iteration 5 - Batch 227/1137 - Train loss: 1.1769539962212245, Train acc: 0.4058388157894737\n",
            "Iteration 6 - Batch 454/1137 - Train loss: 1.1660221845239074, Train acc: 0.4107142857142857\n",
            "Iteration 7 - Batch 681/1137 - Train loss: 1.1599970873325103, Train acc: 0.41209219208211145\n",
            "Iteration 8 - Batch 908/1137 - Train loss: 1.1556559563076536, Train acc: 0.414380500550055\n",
            "Iteration 9 - Batch 1135/1137 - Train loss: 1.1489179996432552, Train acc: 0.41670334507042256\n",
            "Val loss: 1.0811793804168701, Val acc: 0.416\n",
            "Epoch 3/100\n",
            "Iteration 10 - Batch 227/1137 - Train loss: 1.1131696319370938, Train acc: 0.42660361842105265\n",
            "Iteration 11 - Batch 454/1137 - Train loss: 1.1070383709865612, Train acc: 0.43382554945054946\n",
            "Iteration 12 - Batch 681/1137 - Train loss: 1.1078169059194078, Train acc: 0.4359191715542522\n",
            "Iteration 13 - Batch 908/1137 - Train loss: 1.1029372789678793, Train acc: 0.43964865236523654\n",
            "Iteration 14 - Batch 1135/1137 - Train loss: 1.0990544509824733, Train acc: 0.44301551496478875\n",
            "Val loss: 1.019984245300293, Val acc: 0.466\n",
            "Epoch 4/100\n",
            "Iteration 15 - Batch 227/1137 - Train loss: 1.075895920918699, Train acc: 0.4653234649122807\n",
            "Iteration 16 - Batch 454/1137 - Train loss: 1.0714819392005166, Train acc: 0.468853021978022\n",
            "Iteration 17 - Batch 681/1137 - Train loss: 1.069335250473442, Train acc: 0.47156799853372433\n",
            "Iteration 18 - Batch 908/1137 - Train loss: 1.0666832406528712, Train acc: 0.4766742299229923\n",
            "Iteration 19 - Batch 1135/1137 - Train loss: 1.0637426389459992, Train acc: 0.480042363556338\n",
            "Val loss: 1.026314377784729, Val acc: 0.514\n",
            "Epoch 5/100\n",
            "Iteration 20 - Batch 227/1137 - Train loss: 1.0512486419133973, Train acc: 0.49575109649122806\n",
            "Iteration 21 - Batch 454/1137 - Train loss: 1.0436373360864408, Train acc: 0.49903846153846154\n",
            "Iteration 22 - Batch 681/1137 - Train loss: 1.0388289752069457, Train acc: 0.5070564516129032\n",
            "Iteration 23 - Batch 908/1137 - Train loss: 1.0381877207257697, Train acc: 0.5102275852585259\n",
            "Iteration 24 - Batch 1135/1137 - Train loss: 1.0342913567495178, Train acc: 0.5145796654929577\n",
            "Val loss: 1.0044087171554565, Val acc: 0.552\n",
            "Epoch 6/100\n",
            "Iteration 25 - Batch 227/1137 - Train loss: 1.0172598921416098, Train acc: 0.536047149122807\n",
            "Iteration 26 - Batch 454/1137 - Train loss: 1.0164759575665652, Train acc: 0.5385302197802198\n",
            "Iteration 27 - Batch 681/1137 - Train loss: 1.0130762735769545, Train acc: 0.5421325146627566\n",
            "Iteration 28 - Batch 908/1137 - Train loss: 1.0107710966051477, Train acc: 0.5459811606160616\n",
            "Iteration 29 - Batch 1135/1137 - Train loss: 1.0066994065025323, Train acc: 0.549653389084507\n",
            "Val loss: 0.9620925784111023, Val acc: 0.6\n",
            "Epoch 7/100\n",
            "Iteration 30 - Batch 227/1137 - Train loss: 1.009319953228298, Train acc: 0.5651041666666666\n",
            "Iteration 31 - Batch 454/1137 - Train loss: 0.9957749504309434, Train acc: 0.5682005494505494\n",
            "Iteration 32 - Batch 681/1137 - Train loss: 0.9915994638222054, Train acc: 0.5702895894428153\n",
            "Iteration 33 - Batch 908/1137 - Train loss: 0.9890160153407862, Train acc: 0.5729682343234324\n",
            "Iteration 34 - Batch 1135/1137 - Train loss: 0.9868530889212246, Train acc: 0.576323173415493\n",
            "Val loss: 0.9362348318099976, Val acc: 0.61\n",
            "Epoch 8/100\n",
            "Iteration 35 - Batch 227/1137 - Train loss: 0.983561054917804, Train acc: 0.5895696271929824\n",
            "Iteration 36 - Batch 454/1137 - Train loss: 0.9716992950701452, Train acc: 0.5916895604395604\n",
            "Iteration 37 - Batch 681/1137 - Train loss: 0.9705305310987657, Train acc: 0.5929939516129032\n",
            "Iteration 38 - Batch 908/1137 - Train loss: 0.9668721948782079, Train acc: 0.5947125962596259\n",
            "Iteration 39 - Batch 1135/1137 - Train loss: 0.9667423684937013, Train acc: 0.5948090889084507\n",
            "Val loss: 0.9289964437484741, Val acc: 0.638\n",
            "Epoch 9/100\n",
            "Iteration 40 - Batch 227/1137 - Train loss: 0.9516187786010274, Train acc: 0.5990953947368421\n",
            "Iteration 41 - Batch 454/1137 - Train loss: 0.9459271797767053, Train acc: 0.6064217032967033\n",
            "Iteration 42 - Batch 681/1137 - Train loss: 0.9453262719940231, Train acc: 0.6070610337243402\n",
            "Iteration 43 - Batch 908/1137 - Train loss: 0.9458762882697438, Train acc: 0.6099078657865786\n",
            "Iteration 44 - Batch 1135/1137 - Train loss: 0.9438821008087883, Train acc: 0.6109567561619719\n",
            "Val loss: 0.9127805829048157, Val acc: 0.64\n",
            "Epoch 10/100\n",
            "Iteration 45 - Batch 227/1137 - Train loss: 0.9209769362943214, Train acc: 0.6272615131578947\n",
            "Iteration 46 - Batch 454/1137 - Train loss: 0.9232309104322077, Train acc: 0.624793956043956\n",
            "Iteration 47 - Batch 681/1137 - Train loss: 0.9253205009394727, Train acc: 0.6240377565982405\n",
            "Iteration 48 - Batch 908/1137 - Train loss: 0.9249461656487552, Train acc: 0.6249656215621562\n",
            "Iteration 49 - Batch 1135/1137 - Train loss: 0.924648187477404, Train acc: 0.6263754401408451\n",
            "Val loss: 0.9102552533149719, Val acc: 0.646\n",
            "Epoch 11/100\n",
            "Iteration 50 - Batch 227/1137 - Train loss: 0.9064608323469496, Train acc: 0.6424067982456141\n",
            "Iteration 51 - Batch 454/1137 - Train loss: 0.9075083397246979, Train acc: 0.6412087912087913\n",
            "Iteration 52 - Batch 681/1137 - Train loss: 0.9094969602274405, Train acc: 0.6412435850439883\n",
            "Iteration 53 - Batch 908/1137 - Train loss: 0.9049910996744472, Train acc: 0.6445785203520352\n",
            "Iteration 54 - Batch 1135/1137 - Train loss: 0.9021431447439631, Train acc: 0.6448476012323944\n",
            "Val loss: 0.849330723285675, Val acc: 0.65\n",
            "Epoch 12/100\n",
            "Iteration 55 - Batch 227/1137 - Train loss: 0.8963564914046672, Train acc: 0.6491228070175439\n",
            "Iteration 56 - Batch 454/1137 - Train loss: 0.8903097203799657, Train acc: 0.6520947802197802\n",
            "Iteration 57 - Batch 681/1137 - Train loss: 0.8913076949958577, Train acc: 0.6514387829912024\n",
            "Iteration 58 - Batch 908/1137 - Train loss: 0.8855669715485819, Train acc: 0.6539294554455446\n",
            "Iteration 59 - Batch 1135/1137 - Train loss: 0.8842403458457597, Train acc: 0.6544069102112676\n",
            "Val loss: 0.8657585978507996, Val acc: 0.674\n",
            "Epoch 13/100\n",
            "Iteration 60 - Batch 227/1137 - Train loss: 0.8707458868361356, Train acc: 0.6630345394736842\n",
            "Iteration 61 - Batch 454/1137 - Train loss: 0.8694298102305485, Train acc: 0.665625\n",
            "Iteration 62 - Batch 681/1137 - Train loss: 0.8640912265197273, Train acc: 0.6675449046920822\n",
            "Iteration 63 - Batch 908/1137 - Train loss: 0.8625267833897514, Train acc: 0.666133800880088\n",
            "Iteration 64 - Batch 1135/1137 - Train loss: 0.8610990975629276, Train acc: 0.6678587147887324\n",
            "Val loss: 0.822177529335022, Val acc: 0.706\n",
            "Epoch 14/100\n",
            "Iteration 65 - Batch 227/1137 - Train loss: 0.8479466216083158, Train acc: 0.6777686403508771\n",
            "Iteration 66 - Batch 454/1137 - Train loss: 0.8533115776030572, Train acc: 0.6740041208791209\n",
            "Iteration 67 - Batch 681/1137 - Train loss: 0.8519346278783513, Train acc: 0.6745784457478006\n",
            "Iteration 68 - Batch 908/1137 - Train loss: 0.8491805632515709, Train acc: 0.6761207370737073\n",
            "Iteration 69 - Batch 1135/1137 - Train loss: 0.8446125365059141, Train acc: 0.6775968309859155\n",
            "Val loss: 0.7879883050918579, Val acc: 0.704\n",
            "Epoch 15/100\n",
            "Iteration 70 - Batch 227/1137 - Train loss: 0.8389125612744114, Train acc: 0.6773574561403509\n",
            "Iteration 71 - Batch 454/1137 - Train loss: 0.8376800028832404, Train acc: 0.6847184065934065\n",
            "Iteration 72 - Batch 681/1137 - Train loss: 0.8377455858366231, Train acc: 0.6834677419354839\n",
            "Iteration 73 - Batch 908/1137 - Train loss: 0.8332520015168898, Train acc: 0.684784103410341\n",
            "Iteration 74 - Batch 1135/1137 - Train loss: 0.8321444571123157, Train acc: 0.6847078565140845\n",
            "Val loss: 0.8332390189170837, Val acc: 0.688\n",
            "Epoch 16/100\n",
            "Iteration 75 - Batch 227/1137 - Train loss: 0.8246504912773768, Train acc: 0.6842790570175439\n",
            "Iteration 76 - Batch 454/1137 - Train loss: 0.8187385921949869, Train acc: 0.6909684065934066\n",
            "Iteration 77 - Batch 681/1137 - Train loss: 0.8261559642305123, Train acc: 0.6913489736070382\n",
            "Iteration 78 - Batch 908/1137 - Train loss: 0.821390880872183, Train acc: 0.692433305830583\n",
            "Iteration 79 - Batch 1135/1137 - Train loss: 0.8181422617233974, Train acc: 0.6937307438380281\n",
            "Val loss: 0.841255247592926, Val acc: 0.692\n",
            "Epoch 17/100\n",
            "Iteration 80 - Batch 227/1137 - Train loss: 0.8145521034797033, Train acc: 0.6978481359649122\n",
            "Iteration 81 - Batch 454/1137 - Train loss: 0.8065834165929437, Train acc: 0.6985233516483517\n",
            "Iteration 82 - Batch 681/1137 - Train loss: 0.8077097479199384, Train acc: 0.6985657991202346\n",
            "Iteration 83 - Batch 908/1137 - Train loss: 0.8063207655587736, Train acc: 0.700409103410341\n",
            "Iteration 84 - Batch 1135/1137 - Train loss: 0.8062018221113044, Train acc: 0.6996863996478874\n",
            "Val loss: 0.8084973096847534, Val acc: 0.704\n",
            "Epoch 18/100\n",
            "Iteration 85 - Batch 227/1137 - Train loss: 0.8100121387264185, Train acc: 0.6994928728070176\n",
            "Iteration 86 - Batch 454/1137 - Train loss: 0.7994653209225162, Train acc: 0.7029532967032966\n",
            "Iteration 87 - Batch 681/1137 - Train loss: 0.8006422594845121, Train acc: 0.7029646260997068\n",
            "Iteration 88 - Batch 908/1137 - Train loss: 0.7978022863106891, Train acc: 0.7036406765676567\n",
            "Iteration 89 - Batch 1135/1137 - Train loss: 0.7950231305415362, Train acc: 0.7048167913732394\n",
            "Val loss: 0.761469304561615, Val acc: 0.728\n",
            "Epoch 19/100\n",
            "Iteration 90 - Batch 227/1137 - Train loss: 0.7887006425543835, Train acc: 0.7083333333333334\n",
            "Iteration 91 - Batch 454/1137 - Train loss: 0.7906681358159243, Train acc: 0.7077953296703297\n",
            "Iteration 92 - Batch 681/1137 - Train loss: 0.7868666752866985, Train acc: 0.7097232404692082\n",
            "Iteration 93 - Batch 908/1137 - Train loss: 0.7875713932238789, Train acc: 0.711100797579758\n",
            "Iteration 94 - Batch 1135/1137 - Train loss: 0.7853034193247137, Train acc: 0.7131794674295775\n",
            "Val loss: 0.7672194838523865, Val acc: 0.718\n",
            "Epoch 20/100\n",
            "Iteration 95 - Batch 227/1137 - Train loss: 0.7744238959592685, Train acc: 0.7173108552631579\n",
            "Iteration 96 - Batch 454/1137 - Train loss: 0.7753186212791191, Train acc: 0.7166208791208791\n",
            "Iteration 97 - Batch 681/1137 - Train loss: 0.7710154658713299, Train acc: 0.7187729105571847\n",
            "Iteration 98 - Batch 908/1137 - Train loss: 0.7658426489504782, Train acc: 0.7202798404840484\n",
            "Iteration 99 - Batch 1135/1137 - Train loss: 0.7696298936145826, Train acc: 0.7197403169014085\n",
            "Val loss: 0.7736563086509705, Val acc: 0.734\n",
            "Epoch 21/100\n",
            "Iteration 100 - Batch 227/1137 - Train loss: 0.773856932133959, Train acc: 0.7180646929824561\n",
            "Iteration 101 - Batch 454/1137 - Train loss: 0.7645079520377484, Train acc: 0.7218406593406593\n",
            "Iteration 102 - Batch 681/1137 - Train loss: 0.759626417303365, Train acc: 0.7250045821114369\n",
            "Iteration 103 - Batch 908/1137 - Train loss: 0.7598986653867204, Train acc: 0.7241989823982399\n",
            "Iteration 104 - Batch 1135/1137 - Train loss: 0.7636593667344308, Train acc: 0.7235502860915493\n",
            "Val loss: 0.7780574560165405, Val acc: 0.716\n",
            "Epoch 22/100\n",
            "Iteration 105 - Batch 227/1137 - Train loss: 0.7637036172182936, Train acc: 0.7262198464912281\n",
            "Iteration 106 - Batch 454/1137 - Train loss: 0.7581270166150816, Train acc: 0.7276442307692308\n",
            "Iteration 107 - Batch 681/1137 - Train loss: 0.7560590232921836, Train acc: 0.7273414589442815\n",
            "Iteration 108 - Batch 908/1137 - Train loss: 0.7569549830243139, Train acc: 0.7274649339933993\n",
            "Iteration 109 - Batch 1135/1137 - Train loss: 0.754439437001104, Train acc: 0.7287219410211268\n",
            "Val loss: 0.7724711298942566, Val acc: 0.738\n",
            "Epoch 23/100\n",
            "Iteration 110 - Batch 227/1137 - Train loss: 0.7389271761241712, Train acc: 0.7316337719298246\n",
            "Iteration 111 - Batch 454/1137 - Train loss: 0.7466786471042004, Train acc: 0.7296016483516483\n",
            "Iteration 112 - Batch 681/1137 - Train loss: 0.742272233290057, Train acc: 0.7319923020527859\n",
            "Iteration 113 - Batch 908/1137 - Train loss: 0.7450052400430044, Train acc: 0.7320716446644664\n",
            "Iteration 114 - Batch 1135/1137 - Train loss: 0.7444684504675614, Train acc: 0.7330545774647887\n",
            "Val loss: 0.7694135904312134, Val acc: 0.734\n",
            "Epoch 24/100\n",
            "Iteration 115 - Batch 227/1137 - Train loss: 0.7496774315572622, Train acc: 0.7338952850877193\n",
            "Iteration 116 - Batch 454/1137 - Train loss: 0.7413321925388587, Train acc: 0.7354739010989011\n",
            "Iteration 117 - Batch 681/1137 - Train loss: 0.7400227797783016, Train acc: 0.735474706744868\n",
            "Iteration 118 - Batch 908/1137 - Train loss: 0.7378025205746473, Train acc: 0.7360595434543454\n",
            "Iteration 119 - Batch 1135/1137 - Train loss: 0.7363637183648599, Train acc: 0.7355991417253521\n",
            "Val loss: 0.723892092704773, Val acc: 0.74\n",
            "Epoch 25/100\n",
            "Iteration 120 - Batch 227/1137 - Train loss: 0.7317551141768172, Train acc: 0.7432839912280702\n",
            "Iteration 121 - Batch 454/1137 - Train loss: 0.7332299902543917, Train acc: 0.7380151098901099\n",
            "Iteration 122 - Batch 681/1137 - Train loss: 0.7330375153385649, Train acc: 0.7382010630498533\n",
            "Iteration 123 - Batch 908/1137 - Train loss: 0.7315695827091225, Train acc: 0.7383113311331133\n",
            "Iteration 124 - Batch 1135/1137 - Train loss: 0.7305096039732157, Train acc: 0.7385150748239436\n",
            "Val loss: 0.770676851272583, Val acc: 0.742\n",
            "Epoch 26/100\n",
            "Iteration 125 - Batch 227/1137 - Train loss: 0.7204197062212124, Train acc: 0.7407483552631579\n",
            "Iteration 126 - Batch 454/1137 - Train loss: 0.721322178709638, Train acc: 0.7382211538461538\n",
            "Iteration 127 - Batch 681/1137 - Train loss: 0.7228580132957079, Train acc: 0.7386363636363636\n",
            "Iteration 128 - Batch 908/1137 - Train loss: 0.7219196752418648, Train acc: 0.7387066831683168\n",
            "Iteration 129 - Batch 1135/1137 - Train loss: 0.7213444870332597, Train acc: 0.7394366197183099\n",
            "Val loss: 0.7630255818367004, Val acc: 0.734\n",
            "Epoch 27/100\n",
            "Iteration 130 - Batch 227/1137 - Train loss: 0.7016260659224108, Train acc: 0.7486293859649122\n",
            "Iteration 131 - Batch 454/1137 - Train loss: 0.7203436508283511, Train acc: 0.7401785714285715\n",
            "Iteration 132 - Batch 681/1137 - Train loss: 0.7200205640708945, Train acc: 0.7408357771260997\n",
            "Iteration 133 - Batch 908/1137 - Train loss: 0.7175256644341812, Train acc: 0.7415772827282728\n",
            "Iteration 134 - Batch 1135/1137 - Train loss: 0.7157356596591187, Train acc: 0.7430265184859155\n",
            "Val loss: 0.733184278011322, Val acc: 0.756\n",
            "Epoch 28/100\n",
            "Iteration 135 - Batch 227/1137 - Train loss: 0.724216615813866, Train acc: 0.7404742324561403\n",
            "Iteration 136 - Batch 454/1137 - Train loss: 0.718845045697558, Train acc: 0.7416552197802198\n",
            "Iteration 137 - Batch 681/1137 - Train loss: 0.7198259207335386, Train acc: 0.741179435483871\n",
            "Iteration 138 - Batch 908/1137 - Train loss: 0.7155771591220096, Train acc: 0.7425914466446645\n",
            "Iteration 139 - Batch 1135/1137 - Train loss: 0.7140218542337837, Train acc: 0.7433291153169014\n",
            "Val loss: 0.7180234789848328, Val acc: 0.738\n",
            "Epoch 29/100\n",
            "Iteration 140 - Batch 227/1137 - Train loss: 0.7041271681848326, Train acc: 0.743421052631579\n",
            "Iteration 141 - Batch 454/1137 - Train loss: 0.6997538499124758, Train acc: 0.7476991758241758\n",
            "Iteration 142 - Batch 681/1137 - Train loss: 0.702857925788748, Train acc: 0.7458073680351907\n",
            "Iteration 143 - Batch 908/1137 - Train loss: 0.7061323460244765, Train acc: 0.7457198844884488\n",
            "Iteration 144 - Batch 1135/1137 - Train loss: 0.7062116846389753, Train acc: 0.7466851892605634\n",
            "Val loss: 0.7378950119018555, Val acc: 0.748\n",
            "Epoch 30/100\n",
            "Iteration 145 - Batch 227/1137 - Train loss: 0.7012947414789283, Train acc: 0.7491776315789473\n",
            "Iteration 146 - Batch 454/1137 - Train loss: 0.7076471442049677, Train acc: 0.7456387362637362\n",
            "Iteration 147 - Batch 681/1137 - Train loss: 0.7041702023174756, Train acc: 0.7472049120234604\n",
            "Iteration 148 - Batch 908/1137 - Train loss: 0.7037905195627537, Train acc: 0.7476450770077008\n",
            "Iteration 149 - Batch 1135/1137 - Train loss: 0.7028695207194123, Train acc: 0.7485145246478874\n",
            "Val loss: 0.7212172746658325, Val acc: 0.742\n",
            "Epoch 31/100\n",
            "Iteration 150 - Batch 227/1137 - Train loss: 0.6939047639045799, Train acc: 0.7523985745614035\n",
            "Iteration 151 - Batch 454/1137 - Train loss: 0.6942422942800837, Train acc: 0.7508585164835165\n",
            "Iteration 152 - Batch 681/1137 - Train loss: 0.6914442311808511, Train acc: 0.7522910557184751\n",
            "Iteration 153 - Batch 908/1137 - Train loss: 0.6938447416222134, Train acc: 0.7503437843784379\n",
            "Iteration 154 - Batch 1135/1137 - Train loss: 0.6978774815273117, Train acc: 0.7501375440140845\n",
            "Val loss: 0.7818847894668579, Val acc: 0.714\n",
            "Epoch 32/100\n",
            "Iteration 155 - Batch 227/1137 - Train loss: 0.6940954666127238, Train acc: 0.7504797149122807\n",
            "Iteration 156 - Batch 454/1137 - Train loss: 0.6947629494981451, Train acc: 0.7492788461538461\n",
            "Iteration 157 - Batch 681/1137 - Train loss: 0.6958583215377198, Train acc: 0.7511226173020528\n",
            "Iteration 158 - Batch 908/1137 - Train loss: 0.6954034035748774, Train acc: 0.751117299229923\n",
            "Iteration 159 - Batch 1135/1137 - Train loss: 0.6939067832968184, Train acc: 0.7521181778169014\n",
            "Val loss: 0.7440232038497925, Val acc: 0.75\n",
            "Epoch 33/100\n",
            "Iteration 160 - Batch 227/1137 - Train loss: 0.6766967399601351, Train acc: 0.7552768640350878\n",
            "Iteration 161 - Batch 454/1137 - Train loss: 0.6809043210941357, Train acc: 0.7547046703296704\n",
            "Iteration 162 - Batch 681/1137 - Train loss: 0.6812236057348615, Train acc: 0.7550403225806451\n",
            "Iteration 163 - Batch 908/1137 - Train loss: 0.6826855453065257, Train acc: 0.7541769801980198\n",
            "Iteration 164 - Batch 1135/1137 - Train loss: 0.6834981453827988, Train acc: 0.7548553036971831\n",
            "Val loss: 0.7411742806434631, Val acc: 0.718\n",
            "Epoch 34/100\n",
            "Iteration 165 - Batch 227/1137 - Train loss: 0.6876734842856725, Train acc: 0.7537691885964912\n",
            "Iteration 166 - Batch 454/1137 - Train loss: 0.6846176106196183, Train acc: 0.7544986263736264\n",
            "Iteration 167 - Batch 681/1137 - Train loss: 0.6866510683927368, Train acc: 0.7540780791788856\n",
            "Iteration 168 - Batch 908/1137 - Train loss: 0.6819369343772318, Train acc: 0.7557583883388339\n",
            "Iteration 169 - Batch 1135/1137 - Train loss: 0.6833229862993032, Train acc: 0.7553779709507042\n",
            "Val loss: 0.7072641849517822, Val acc: 0.73\n",
            "Epoch 35/100\n",
            "Iteration 170 - Batch 227/1137 - Train loss: 0.6642511562820066, Train acc: 0.7669956140350878\n",
            "Iteration 171 - Batch 454/1137 - Train loss: 0.6710728954482864, Train acc: 0.7616758241758241\n",
            "Iteration 172 - Batch 681/1137 - Train loss: 0.671762166004027, Train acc: 0.7617531158357771\n",
            "Iteration 173 - Batch 908/1137 - Train loss: 0.6733588671395749, Train acc: 0.7606229372937293\n",
            "Iteration 174 - Batch 1135/1137 - Train loss: 0.6771915073598355, Train acc: 0.7588853433098591\n",
            "Val loss: 0.7409965395927429, Val acc: 0.732\n",
            "Epoch 36/100\n",
            "Iteration 175 - Batch 227/1137 - Train loss: 0.6783647797348207, Train acc: 0.7569901315789473\n",
            "Iteration 176 - Batch 454/1137 - Train loss: 0.6769787165489826, Train acc: 0.7588598901098901\n",
            "Iteration 177 - Batch 681/1137 - Train loss: 0.6734817943527551, Train acc: 0.7586143695014663\n",
            "Iteration 178 - Batch 908/1137 - Train loss: 0.6745626460535429, Train acc: 0.7584055280528053\n",
            "Iteration 179 - Batch 1135/1137 - Train loss: 0.6746581384885899, Train acc: 0.7578262544014085\n",
            "Val loss: 0.7046094536781311, Val acc: 0.762\n",
            "Epoch 37/100\n",
            "Iteration 180 - Batch 227/1137 - Train loss: 0.6643683893377321, Train acc: 0.7600054824561403\n",
            "Iteration 181 - Batch 454/1137 - Train loss: 0.6668391788398826, Train acc: 0.7586195054945055\n",
            "Iteration 182 - Batch 681/1137 - Train loss: 0.6728504515113719, Train acc: 0.7574917521994134\n",
            "Iteration 183 - Batch 908/1137 - Train loss: 0.6718470279467512, Train acc: 0.7587493124312431\n",
            "Iteration 184 - Batch 1135/1137 - Train loss: 0.6711587179452181, Train acc: 0.7593529929577465\n",
            "Val loss: 0.7470173835754395, Val acc: 0.748\n",
            "Epoch 38/100\n",
            "Iteration 185 - Batch 227/1137 - Train loss: 0.6709292805508563, Train acc: 0.7599369517543859\n",
            "Iteration 186 - Batch 454/1137 - Train loss: 0.6735461442025154, Train acc: 0.7581043956043956\n",
            "Iteration 187 - Batch 681/1137 - Train loss: 0.6700822304595601, Train acc: 0.7596682551319648\n",
            "Iteration 188 - Batch 908/1137 - Train loss: 0.6700587488046967, Train acc: 0.7595056380638063\n",
            "Iteration 189 - Batch 1135/1137 - Train loss: 0.6685638442950349, Train acc: 0.7602332746478874\n",
            "Val loss: 0.7585051655769348, Val acc: 0.752\n",
            "Epoch 39/100\n",
            "Iteration 190 - Batch 227/1137 - Train loss: 0.6602148400586948, Train acc: 0.7644599780701754\n",
            "Iteration 191 - Batch 454/1137 - Train loss: 0.6613741456807315, Train acc: 0.7637362637362637\n",
            "Iteration 192 - Batch 681/1137 - Train loss: 0.6663332226688911, Train acc: 0.7617531158357771\n",
            "Iteration 193 - Batch 908/1137 - Train loss: 0.6658082295893574, Train acc: 0.7618777502750275\n",
            "Iteration 194 - Batch 1135/1137 - Train loss: 0.6629855708074821, Train acc: 0.7626953125\n",
            "Val loss: 0.7602521777153015, Val acc: 0.742\n",
            "Epoch 40/100\n",
            "Iteration 195 - Batch 227/1137 - Train loss: 0.6732062296124927, Train acc: 0.7630208333333334\n",
            "Iteration 196 - Batch 454/1137 - Train loss: 0.6634514651455722, Train acc: 0.7641483516483516\n",
            "Iteration 197 - Batch 681/1137 - Train loss: 0.6655171792790338, Train acc: 0.7628070014662757\n",
            "Iteration 198 - Batch 908/1137 - Train loss: 0.6653126873264481, Train acc: 0.7623934268426843\n",
            "Iteration 199 - Batch 1135/1137 - Train loss: 0.6636965401260786, Train acc: 0.7631904709507042\n",
            "Val loss: 0.7116308212280273, Val acc: 0.75\n",
            "Epoch 41/100\n",
            "Iteration 200 - Batch 227/1137 - Train loss: 0.6637188392250162, Train acc: 0.7636376096491229\n",
            "Iteration 201 - Batch 454/1137 - Train loss: 0.6603834803287799, Train acc: 0.7642513736263736\n",
            "Iteration 202 - Batch 681/1137 - Train loss: 0.6623473366684228, Train acc: 0.7638608870967742\n",
            "Iteration 203 - Batch 908/1137 - Train loss: 0.6585444966361861, Train acc: 0.7654359185918592\n",
            "Iteration 204 - Batch 1135/1137 - Train loss: 0.6575807453146283, Train acc: 0.7656387544014085\n",
            "Val loss: 0.6838572025299072, Val acc: 0.764\n",
            "Epoch 42/100\n",
            "Iteration 205 - Batch 227/1137 - Train loss: 0.6579162219637319, Train acc: 0.7650767543859649\n",
            "Iteration 206 - Batch 454/1137 - Train loss: 0.6476983471886142, Train acc: 0.7685096153846154\n",
            "Iteration 207 - Batch 681/1137 - Train loss: 0.6552026208894344, Train acc: 0.7661977639296188\n",
            "Iteration 208 - Batch 908/1137 - Train loss: 0.6519822503867323, Train acc: 0.7685987348734874\n",
            "Iteration 209 - Batch 1135/1137 - Train loss: 0.65327258670414, Train acc: 0.7682658450704225\n",
            "Val loss: 0.7383971214294434, Val acc: 0.75\n",
            "Epoch 43/100\n",
            "Iteration 210 - Batch 227/1137 - Train loss: 0.6520188054756114, Train acc: 0.7701480263157895\n",
            "Iteration 211 - Batch 454/1137 - Train loss: 0.6477772357044639, Train acc: 0.7699519230769231\n",
            "Iteration 212 - Batch 681/1137 - Train loss: 0.6497724940262931, Train acc: 0.7684429985337243\n",
            "Iteration 213 - Batch 908/1137 - Train loss: 0.6526762389376087, Train acc: 0.7675673817381738\n",
            "Iteration 214 - Batch 1135/1137 - Train loss: 0.6504685949848991, Train acc: 0.7677431778169014\n",
            "Val loss: 0.753648042678833, Val acc: 0.782\n",
            "Epoch 44/100\n",
            "Iteration 215 - Batch 227/1137 - Train loss: 0.6530997324408147, Train acc: 0.7677494517543859\n",
            "Iteration 216 - Batch 454/1137 - Train loss: 0.655353966322574, Train acc: 0.7678914835164835\n",
            "Iteration 217 - Batch 681/1137 - Train loss: 0.6507324080837786, Train acc: 0.7699550953079178\n",
            "Iteration 218 - Batch 908/1137 - Train loss: 0.6469759401774118, Train acc: 0.7701285753575358\n",
            "Iteration 219 - Batch 1135/1137 - Train loss: 0.6463427694225815, Train acc: 0.7707966549295775\n",
            "Val loss: 0.6945877075195312, Val acc: 0.762\n",
            "Epoch 45/100\n",
            "Iteration 220 - Batch 227/1137 - Train loss: 0.6432877488826451, Train acc: 0.772203947368421\n",
            "Iteration 221 - Batch 454/1137 - Train loss: 0.6433890604055845, Train acc: 0.7727335164835165\n",
            "Iteration 222 - Batch 681/1137 - Train loss: 0.6471628126772967, Train acc: 0.7717192082111437\n",
            "Iteration 223 - Batch 908/1137 - Train loss: 0.6482777502736111, Train acc: 0.7711771177117712\n",
            "Iteration 224 - Batch 1135/1137 - Train loss: 0.6488904384764987, Train acc: 0.7703290052816901\n",
            "Val loss: 0.7195090055465698, Val acc: 0.752\n",
            "Epoch 46/100\n",
            "Iteration 225 - Batch 227/1137 - Train loss: 0.6507277284797869, Train acc: 0.7662417763157895\n",
            "Iteration 226 - Batch 454/1137 - Train loss: 0.6470657627006153, Train acc: 0.770260989010989\n",
            "Iteration 227 - Batch 681/1137 - Train loss: 0.6513231947275201, Train acc: 0.7681909824046921\n",
            "Iteration 228 - Batch 908/1137 - Train loss: 0.6474633420130076, Train acc: 0.7691659790979097\n",
            "Iteration 229 - Batch 1135/1137 - Train loss: 0.6490863106462738, Train acc: 0.7685134242957746\n",
            "Val loss: 0.7423691749572754, Val acc: 0.754\n",
            "Epoch 47/100\n",
            "Iteration 230 - Batch 227/1137 - Train loss: 0.6314869068170849, Train acc: 0.7756990131578947\n",
            "Iteration 231 - Batch 454/1137 - Train loss: 0.6342699939078027, Train acc: 0.7768887362637362\n",
            "Iteration 232 - Batch 681/1137 - Train loss: 0.633314422017668, Train acc: 0.7768282624633431\n",
            "Iteration 233 - Batch 908/1137 - Train loss: 0.6344224082426925, Train acc: 0.7761791804180418\n",
            "Iteration 234 - Batch 1135/1137 - Train loss: 0.6361731520106255, Train acc: 0.7748679577464789\n",
            "Val loss: 0.7282478213310242, Val acc: 0.766\n",
            "Epoch 48/100\n",
            "Iteration 235 - Batch 227/1137 - Train loss: 0.6374370908005196, Train acc: 0.7753563596491229\n",
            "Iteration 236 - Batch 454/1137 - Train loss: 0.6394828832411504, Train acc: 0.7722184065934066\n",
            "Iteration 237 - Batch 681/1137 - Train loss: 0.6331027503674331, Train acc: 0.774445564516129\n",
            "Iteration 238 - Batch 908/1137 - Train loss: 0.6342661675840321, Train acc: 0.7748384213421342\n",
            "Iteration 239 - Batch 1135/1137 - Train loss: 0.6346416502246555, Train acc: 0.7748679577464789\n",
            "Val loss: 0.7134721279144287, Val acc: 0.756\n",
            "Epoch 49/100\n",
            "Iteration 240 - Batch 227/1137 - Train loss: 0.633465824419992, Train acc: 0.7754248903508771\n",
            "Iteration 241 - Batch 454/1137 - Train loss: 0.6352826611681299, Train acc: 0.7730425824175824\n",
            "Iteration 242 - Batch 681/1137 - Train loss: 0.6331206028762102, Train acc: 0.7736895161290323\n",
            "Iteration 243 - Batch 908/1137 - Train loss: 0.6333456505643259, Train acc: 0.7736695544554455\n",
            "Iteration 244 - Batch 1135/1137 - Train loss: 0.6349466297723038, Train acc: 0.7736163072183099\n",
            "Val loss: 0.7410517334938049, Val acc: 0.766\n",
            "Epoch 50/100\n",
            "Iteration 245 - Batch 227/1137 - Train loss: 0.634060007569037, Train acc: 0.7740542763157895\n",
            "Iteration 246 - Batch 454/1137 - Train loss: 0.6299917496167696, Train acc: 0.7753090659340659\n",
            "Iteration 247 - Batch 681/1137 - Train loss: 0.6321157100350976, Train acc: 0.7765762463343109\n",
            "Iteration 248 - Batch 908/1137 - Train loss: 0.6315243460229784, Train acc: 0.7767120462046204\n",
            "Iteration 249 - Batch 1135/1137 - Train loss: 0.630286051458876, Train acc: 0.7767385563380281\n",
            "Val loss: 0.7058376669883728, Val acc: 0.754\n",
            "Epoch 51/100\n",
            "Iteration 250 - Batch 227/1137 - Train loss: 0.6253826520160625, Train acc: 0.7805646929824561\n",
            "Iteration 251 - Batch 454/1137 - Train loss: 0.6314418387282026, Train acc: 0.7782623626373626\n",
            "Iteration 252 - Batch 681/1137 - Train loss: 0.634711437692041, Train acc: 0.7761867668621701\n",
            "Iteration 253 - Batch 908/1137 - Train loss: 0.6307652101199357, Train acc: 0.7775715071507151\n",
            "Iteration 254 - Batch 1135/1137 - Train loss: 0.6285440834160422, Train acc: 0.7783753301056338\n",
            "Val loss: 0.7396494746208191, Val acc: 0.75\n",
            "Epoch 52/100\n",
            "Iteration 255 - Batch 227/1137 - Train loss: 0.6264902061799116, Train acc: 0.7780290570175439\n",
            "Iteration 256 - Batch 454/1137 - Train loss: 0.6203781542542216, Train acc: 0.7796016483516484\n",
            "Iteration 257 - Batch 681/1137 - Train loss: 0.6211019632491198, Train acc: 0.778775659824047\n",
            "Iteration 258 - Batch 908/1137 - Train loss: 0.6241741831546569, Train acc: 0.7787231848184818\n",
            "Iteration 259 - Batch 1135/1137 - Train loss: 0.6251747220964499, Train acc: 0.7785541373239436\n",
            "Val loss: 0.7111744284629822, Val acc: 0.77\n",
            "Epoch 53/100\n",
            "Iteration 260 - Batch 227/1137 - Train loss: 0.6149031557796294, Train acc: 0.7846765350877193\n",
            "Iteration 261 - Batch 454/1137 - Train loss: 0.6208259673563988, Train acc: 0.7834821428571429\n",
            "Iteration 262 - Batch 681/1137 - Train loss: 0.6195954077841599, Train acc: 0.7835639662756598\n",
            "Iteration 263 - Batch 908/1137 - Train loss: 0.6205164350167621, Train acc: 0.7825048129812981\n",
            "Iteration 264 - Batch 1135/1137 - Train loss: 0.6210002372644736, Train acc: 0.7826391945422535\n",
            "Val loss: 0.7069260478019714, Val acc: 0.78\n",
            "Epoch 54/100\n",
            "Iteration 265 - Batch 227/1137 - Train loss: 0.6225061953852051, Train acc: 0.7791940789473685\n",
            "Iteration 266 - Batch 454/1137 - Train loss: 0.6245573630044748, Train acc: 0.7785714285714286\n",
            "Iteration 267 - Batch 681/1137 - Train loss: 0.6223977554665983, Train acc: 0.7796462609970675\n",
            "Iteration 268 - Batch 908/1137 - Train loss: 0.622223759215526, Train acc: 0.779651402640264\n",
            "Iteration 269 - Batch 1135/1137 - Train loss: 0.6192125785623638, Train acc: 0.780823613556338\n",
            "Val loss: 0.7755081057548523, Val acc: 0.74\n",
            "Epoch 55/100\n",
            "Iteration 270 - Batch 227/1137 - Train loss: 0.6119020634837318, Train acc: 0.78515625\n",
            "Iteration 271 - Batch 454/1137 - Train loss: 0.6179251410803952, Train acc: 0.7808722527472527\n",
            "Iteration 272 - Batch 681/1137 - Train loss: 0.6139907523159407, Train acc: 0.7813645527859238\n",
            "Iteration 273 - Batch 908/1137 - Train loss: 0.6132648863611442, Train acc: 0.7824188668866887\n",
            "Iteration 274 - Batch 1135/1137 - Train loss: 0.6139696678802581, Train acc: 0.7821715448943662\n",
            "Val loss: 0.7247059345245361, Val acc: 0.756\n",
            "Epoch 56/100\n",
            "Iteration 275 - Batch 227/1137 - Train loss: 0.6147073238042363, Train acc: 0.7857044956140351\n",
            "Iteration 276 - Batch 454/1137 - Train loss: 0.6090635378282149, Train acc: 0.7868131868131868\n",
            "Iteration 277 - Batch 681/1137 - Train loss: 0.6094338322752032, Train acc: 0.7857175586510264\n",
            "Iteration 278 - Batch 908/1137 - Train loss: 0.6118235110711999, Train acc: 0.7847222222222222\n",
            "Iteration 279 - Batch 1135/1137 - Train loss: 0.6115718910234494, Train acc: 0.7857889524647887\n",
            "Val loss: 0.751825749874115, Val acc: 0.752\n",
            "Epoch 57/100\n",
            "Iteration 280 - Batch 227/1137 - Train loss: 0.609162143317231, Train acc: 0.7824150219298246\n",
            "Iteration 281 - Batch 454/1137 - Train loss: 0.6063740399512616, Train acc: 0.7845123626373627\n",
            "Iteration 282 - Batch 681/1137 - Train loss: 0.6073646493409036, Train acc: 0.7837701612903226\n",
            "Iteration 283 - Batch 908/1137 - Train loss: 0.6070882348534298, Train acc: 0.7844815731573157\n",
            "Iteration 284 - Batch 1135/1137 - Train loss: 0.6047435912867667, Train acc: 0.7855963908450704\n",
            "Val loss: 0.7041872143745422, Val acc: 0.74\n",
            "Epoch 58/100\n",
            "Iteration 285 - Batch 227/1137 - Train loss: 0.6069967422569007, Train acc: 0.7839912280701754\n",
            "Iteration 286 - Batch 454/1137 - Train loss: 0.6023533931800298, Train acc: 0.7857142857142857\n",
            "Iteration 287 - Batch 681/1137 - Train loss: 0.604397120145409, Train acc: 0.7854197214076246\n",
            "Iteration 288 - Batch 908/1137 - Train loss: 0.6054557956615821, Train acc: 0.785564493949395\n",
            "Iteration 289 - Batch 1135/1137 - Train loss: 0.6067041020682999, Train acc: 0.7854863556338029\n",
            "Val loss: 0.7395226359367371, Val acc: 0.758\n",
            "Epoch 59/100\n",
            "Iteration 290 - Batch 227/1137 - Train loss: 0.5976071901488722, Train acc: 0.7911184210526315\n",
            "Iteration 291 - Batch 454/1137 - Train loss: 0.5979635418771387, Train acc: 0.7910027472527472\n",
            "Iteration 292 - Batch 681/1137 - Train loss: 0.6001814940283375, Train acc: 0.7895436217008798\n",
            "Iteration 293 - Batch 908/1137 - Train loss: 0.6003178065628371, Train acc: 0.7888648239823982\n",
            "Iteration 294 - Batch 1135/1137 - Train loss: 0.5980927076610462, Train acc: 0.7897364656690141\n",
            "Val loss: 0.733144223690033, Val acc: 0.752\n",
            "Epoch 60/100\n",
            "Iteration 295 - Batch 227/1137 - Train loss: 0.5886312734923864, Train acc: 0.7929002192982456\n",
            "Iteration 296 - Batch 454/1137 - Train loss: 0.5919826685727297, Train acc: 0.7913461538461538\n",
            "Iteration 297 - Batch 681/1137 - Train loss: 0.5981197056095621, Train acc: 0.7894061583577713\n",
            "Iteration 298 - Batch 908/1137 - Train loss: 0.5967039857367085, Train acc: 0.7899821232123212\n",
            "Iteration 299 - Batch 1135/1137 - Train loss: 0.596641305361835, Train acc: 0.790162852112676\n",
            "Val loss: 0.670680820941925, Val acc: 0.78\n",
            "Epoch 61/100\n",
            "Iteration 300 - Batch 227/1137 - Train loss: 0.5991539557774862, Train acc: 0.7911184210526315\n",
            "Iteration 301 - Batch 454/1137 - Train loss: 0.6046836166591435, Train acc: 0.7881524725274726\n",
            "Iteration 302 - Batch 681/1137 - Train loss: 0.5981159427386226, Train acc: 0.7905058651026393\n",
            "Iteration 303 - Batch 908/1137 - Train loss: 0.5940484380958104, Train acc: 0.7905665566556656\n",
            "Iteration 304 - Batch 1135/1137 - Train loss: 0.5928479390822246, Train acc: 0.7905754841549296\n",
            "Val loss: 0.7742470502853394, Val acc: 0.756\n",
            "Epoch 62/100\n",
            "Iteration 305 - Batch 227/1137 - Train loss: 0.5703504231938145, Train acc: 0.80078125\n",
            "Iteration 306 - Batch 454/1137 - Train loss: 0.5800988057157496, Train acc: 0.7967376373626374\n",
            "Iteration 307 - Batch 681/1137 - Train loss: 0.5839203569022092, Train acc: 0.7946068548387096\n",
            "Iteration 308 - Batch 908/1137 - Train loss: 0.5848933263252539, Train acc: 0.7942794279427943\n",
            "Iteration 309 - Batch 1135/1137 - Train loss: 0.5884409830446394, Train acc: 0.7927211707746479\n",
            "Val loss: 0.7116429805755615, Val acc: 0.762\n",
            "Epoch 63/100\n",
            "Iteration 310 - Batch 227/1137 - Train loss: 0.5861279488655559, Train acc: 0.7924890350877193\n",
            "Iteration 311 - Batch 454/1137 - Train loss: 0.5814346760838897, Train acc: 0.79375\n",
            "Iteration 312 - Batch 681/1137 - Train loss: 0.5821556367220417, Train acc: 0.7942173753665689\n",
            "Iteration 313 - Batch 908/1137 - Train loss: 0.5833897220813008, Train acc: 0.7936606160616062\n",
            "Iteration 314 - Batch 1135/1137 - Train loss: 0.5886422596399633, Train acc: 0.7935326804577465\n",
            "Val loss: 0.720303475856781, Val acc: 0.774\n",
            "Epoch 64/100\n",
            "Iteration 315 - Batch 227/1137 - Train loss: 0.5838547133301434, Train acc: 0.7960526315789473\n",
            "Iteration 316 - Batch 454/1137 - Train loss: 0.5834820585591453, Train acc: 0.7952266483516484\n",
            "Iteration 317 - Batch 681/1137 - Train loss: 0.5806644615714501, Train acc: 0.7963938782991202\n",
            "Iteration 318 - Batch 908/1137 - Train loss: 0.5796597952430922, Train acc: 0.7970984598459846\n",
            "Iteration 319 - Batch 1135/1137 - Train loss: 0.5823610033315252, Train acc: 0.7957884022887324\n",
            "Val loss: 0.7554876208305359, Val acc: 0.774\n",
            "Epoch 65/100\n",
            "Iteration 320 - Batch 227/1137 - Train loss: 0.5940211117267609, Train acc: 0.7957785087719298\n",
            "Iteration 321 - Batch 454/1137 - Train loss: 0.5854820208890098, Train acc: 0.7970467032967034\n",
            "Iteration 322 - Batch 681/1137 - Train loss: 0.5862462913011176, Train acc: 0.7969437316715543\n",
            "Iteration 323 - Batch 908/1137 - Train loss: 0.5839669992952589, Train acc: 0.7969609460946094\n",
            "Iteration 324 - Batch 1135/1137 - Train loss: 0.5832076716223653, Train acc: 0.7969300176056338\n",
            "Val loss: 0.7597751617431641, Val acc: 0.76\n",
            "Epoch 66/100\n",
            "Iteration 325 - Batch 227/1137 - Train loss: 0.5861326755400289, Train acc: 0.7935169956140351\n",
            "Iteration 326 - Batch 454/1137 - Train loss: 0.58465677519421, Train acc: 0.7936813186813186\n",
            "Iteration 327 - Batch 681/1137 - Train loss: 0.5827001863735513, Train acc: 0.7952712609970675\n",
            "Iteration 328 - Batch 908/1137 - Train loss: 0.5820793180969288, Train acc: 0.7957233223322332\n",
            "Iteration 329 - Batch 1135/1137 - Train loss: 0.5807260997383528, Train acc: 0.7963798415492958\n",
            "Val loss: 0.7357186675071716, Val acc: 0.768\n",
            "Epoch 67/100\n",
            "Iteration 330 - Batch 227/1137 - Train loss: 0.5733430201939324, Train acc: 0.7974917763157895\n",
            "Iteration 331 - Batch 454/1137 - Train loss: 0.5705081936094787, Train acc: 0.8001373626373627\n",
            "Iteration 332 - Batch 681/1137 - Train loss: 0.5736464348313046, Train acc: 0.8000824780058651\n",
            "Iteration 333 - Batch 908/1137 - Train loss: 0.5785579254361006, Train acc: 0.7978891639163916\n",
            "Iteration 334 - Batch 1135/1137 - Train loss: 0.5807825750999258, Train acc: 0.7965861575704225\n",
            "Val loss: 0.7165502309799194, Val acc: 0.752\n",
            "Epoch 68/100\n",
            "Iteration 335 - Batch 227/1137 - Train loss: 0.5767058127543383, Train acc: 0.7970805921052632\n",
            "Iteration 336 - Batch 454/1137 - Train loss: 0.5728809314769703, Train acc: 0.7987293956043956\n",
            "Iteration 337 - Batch 681/1137 - Train loss: 0.5735150751742449, Train acc: 0.7984329178885631\n",
            "Iteration 338 - Batch 908/1137 - Train loss: 0.5722096241412252, Train acc: 0.7991439768976898\n",
            "Iteration 339 - Batch 1135/1137 - Train loss: 0.5738286175568339, Train acc: 0.7989381602112676\n",
            "Val loss: 0.7131918668746948, Val acc: 0.772\n",
            "Epoch 69/100\n",
            "Iteration 340 - Batch 227/1137 - Train loss: 0.5755638190005955, Train acc: 0.7967379385964912\n",
            "Iteration 341 - Batch 454/1137 - Train loss: 0.57226046533375, Train acc: 0.798489010989011\n",
            "Iteration 342 - Batch 681/1137 - Train loss: 0.5776471629782506, Train acc: 0.797516495601173\n",
            "Iteration 343 - Batch 908/1137 - Train loss: 0.5768536825319066, Train acc: 0.7979407315731574\n",
            "Iteration 344 - Batch 1135/1137 - Train loss: 0.575664537971918, Train acc: 0.7980166153169014\n",
            "Val loss: 0.7408599853515625, Val acc: 0.728\n",
            "Epoch 70/100\n",
            "Iteration 345 - Batch 227/1137 - Train loss: 0.571038761029118, Train acc: 0.7992050438596491\n",
            "Iteration 346 - Batch 454/1137 - Train loss: 0.5766749354509207, Train acc: 0.796978021978022\n",
            "Iteration 347 - Batch 681/1137 - Train loss: 0.5763166294244727, Train acc: 0.797883064516129\n",
            "Iteration 348 - Batch 908/1137 - Train loss: 0.5732250049366022, Train acc: 0.7989892739273927\n",
            "Iteration 349 - Batch 1135/1137 - Train loss: 0.5723960788684411, Train acc: 0.7996396346830986\n",
            "Val loss: 0.7693180441856384, Val acc: 0.768\n",
            "Epoch 71/100\n",
            "Iteration 350 - Batch 227/1137 - Train loss: 0.5768041766264982, Train acc: 0.8003015350877193\n",
            "Iteration 351 - Batch 454/1137 - Train loss: 0.575700888463429, Train acc: 0.7995535714285714\n",
            "Iteration 352 - Batch 681/1137 - Train loss: 0.5749409731007741, Train acc: 0.7998762829912024\n",
            "Iteration 353 - Batch 908/1137 - Train loss: 0.5722289675205323, Train acc: 0.8001581408140814\n",
            "Iteration 354 - Batch 1135/1137 - Train loss: 0.5688562818711073, Train acc: 0.8008775308098591\n",
            "Val loss: 0.6746917366981506, Val acc: 0.774\n",
            "Epoch 72/100\n",
            "Iteration 355 - Batch 227/1137 - Train loss: 0.5669039338827133, Train acc: 0.8017406798245614\n",
            "Iteration 356 - Batch 454/1137 - Train loss: 0.565494389127899, Train acc: 0.8023351648351649\n",
            "Iteration 357 - Batch 681/1137 - Train loss: 0.5676631077118983, Train acc: 0.8018007697947214\n",
            "Iteration 358 - Batch 908/1137 - Train loss: 0.5662812500765877, Train acc: 0.802306793179318\n",
            "Iteration 359 - Batch 1135/1137 - Train loss: 0.5672026108950377, Train acc: 0.8020741637323944\n",
            "Val loss: 0.7511062622070312, Val acc: 0.764\n",
            "Epoch 73/100\n",
            "Iteration 360 - Batch 227/1137 - Train loss: 0.5611465536711508, Train acc: 0.8003015350877193\n",
            "Iteration 361 - Batch 454/1137 - Train loss: 0.5622042473855909, Train acc: 0.8024725274725275\n",
            "Iteration 362 - Batch 681/1137 - Train loss: 0.5652493501513585, Train acc: 0.8014571114369502\n",
            "Iteration 363 - Batch 908/1137 - Train loss: 0.5635026040250318, Train acc: 0.8025646314631463\n",
            "Iteration 364 - Batch 1135/1137 - Train loss: 0.5651286948787074, Train acc: 0.8026793573943662\n",
            "Val loss: 0.6694006323814392, Val acc: 0.774\n",
            "Epoch 74/100\n",
            "Iteration 365 - Batch 227/1137 - Train loss: 0.5746880411578897, Train acc: 0.8013294956140351\n",
            "Iteration 366 - Batch 454/1137 - Train loss: 0.5692367967013474, Train acc: 0.8032623626373626\n",
            "Iteration 367 - Batch 681/1137 - Train loss: 0.5657336341320944, Train acc: 0.804779142228739\n",
            "Iteration 368 - Batch 908/1137 - Train loss: 0.5615567172517871, Train acc: 0.8059165291529153\n",
            "Iteration 369 - Batch 1135/1137 - Train loss: 0.5602551531571318, Train acc: 0.8060629401408451\n",
            "Val loss: 0.6871885657310486, Val acc: 0.782\n",
            "Epoch 75/100\n",
            "Iteration 370 - Batch 227/1137 - Train loss: 0.5609859331396588, Train acc: 0.8050301535087719\n",
            "Iteration 371 - Batch 454/1137 - Train loss: 0.5645739508199168, Train acc: 0.8028846153846154\n",
            "Iteration 372 - Batch 681/1137 - Train loss: 0.5620980640310695, Train acc: 0.8029692082111437\n",
            "Iteration 373 - Batch 908/1137 - Train loss: 0.5605242928393734, Train acc: 0.8039053905390539\n",
            "Iteration 374 - Batch 1135/1137 - Train loss: 0.561652903746761, Train acc: 0.8038897447183099\n",
            "Val loss: 0.7207759618759155, Val acc: 0.752\n",
            "Epoch 76/100\n",
            "Iteration 375 - Batch 227/1137 - Train loss: 0.571033127736627, Train acc: 0.8020833333333334\n",
            "Iteration 376 - Batch 454/1137 - Train loss: 0.5654168765623491, Train acc: 0.8023008241758242\n",
            "Iteration 377 - Batch 681/1137 - Train loss: 0.559954015478011, Train acc: 0.8046875\n",
            "Iteration 378 - Batch 908/1137 - Train loss: 0.5583611151041633, Train acc: 0.805641501650165\n",
            "Iteration 379 - Batch 1135/1137 - Train loss: 0.557690751122337, Train acc: 0.8061317121478874\n",
            "Val loss: 0.7365190386772156, Val acc: 0.784\n",
            "Epoch 77/100\n",
            "Iteration 380 - Batch 227/1137 - Train loss: 0.5590333359544737, Train acc: 0.8014665570175439\n",
            "Iteration 381 - Batch 454/1137 - Train loss: 0.5623624470207718, Train acc: 0.8032967032967033\n",
            "Iteration 382 - Batch 681/1137 - Train loss: 0.5607182735746558, Train acc: 0.8046187683284457\n",
            "Iteration 383 - Batch 908/1137 - Train loss: 0.5589449167645016, Train acc: 0.8056930693069307\n",
            "Iteration 384 - Batch 1135/1137 - Train loss: 0.5604703958657845, Train acc: 0.8053064480633803\n",
            "Val loss: 0.7369496822357178, Val acc: 0.76\n",
            "Epoch 78/100\n",
            "Iteration 385 - Batch 227/1137 - Train loss: 0.5508100202209071, Train acc: 0.8076343201754386\n",
            "Iteration 386 - Batch 454/1137 - Train loss: 0.5529159188270569, Train acc: 0.8064903846153846\n",
            "Iteration 387 - Batch 681/1137 - Train loss: 0.5543887072906466, Train acc: 0.8064516129032258\n",
            "Iteration 388 - Batch 908/1137 - Train loss: 0.5555210331551181, Train acc: 0.8057274477447744\n",
            "Iteration 389 - Batch 1135/1137 - Train loss: 0.5555027762401692, Train acc: 0.8060629401408451\n",
            "Val loss: 0.7053616046905518, Val acc: 0.766\n",
            "Epoch 79/100\n",
            "Iteration 390 - Batch 227/1137 - Train loss: 0.5515948654266826, Train acc: 0.8101014254385965\n",
            "Iteration 391 - Batch 454/1137 - Train loss: 0.5525372961720267, Train acc: 0.8091346153846154\n",
            "Iteration 392 - Batch 681/1137 - Train loss: 0.5497744550977635, Train acc: 0.8105755131964809\n",
            "Iteration 393 - Batch 908/1137 - Train loss: 0.5532057120396097, Train acc: 0.8087871287128713\n",
            "Iteration 394 - Batch 1135/1137 - Train loss: 0.553118407017958, Train acc: 0.8085662411971831\n",
            "Val loss: 0.6578230261802673, Val acc: 0.788\n",
            "Epoch 80/100\n",
            "Iteration 395 - Batch 227/1137 - Train loss: 0.5468797598753059, Train acc: 0.8054413377192983\n",
            "Iteration 396 - Batch 454/1137 - Train loss: 0.5505796588384189, Train acc: 0.807657967032967\n",
            "Iteration 397 - Batch 681/1137 - Train loss: 0.5497886263205509, Train acc: 0.8068181818181818\n",
            "Iteration 398 - Batch 908/1137 - Train loss: 0.5500372811184727, Train acc: 0.8074119911991199\n",
            "Iteration 399 - Batch 1135/1137 - Train loss: 0.5514218300633448, Train acc: 0.8071082746478874\n",
            "Val loss: 0.7256530523300171, Val acc: 0.772\n",
            "Epoch 81/100\n",
            "Iteration 400 - Batch 227/1137 - Train loss: 0.5499490554395475, Train acc: 0.809827302631579\n",
            "Iteration 401 - Batch 454/1137 - Train loss: 0.5593360769224691, Train acc: 0.8064903846153846\n",
            "Iteration 402 - Batch 681/1137 - Train loss: 0.5558355741661665, Train acc: 0.8068640029325513\n",
            "Iteration 403 - Batch 908/1137 - Train loss: 0.5530919363461968, Train acc: 0.8087355610561056\n",
            "Iteration 404 - Batch 1135/1137 - Train loss: 0.5506150008664584, Train acc: 0.8090751540492958\n",
            "Val loss: 0.7354556918144226, Val acc: 0.776\n",
            "Epoch 82/100\n",
            "Iteration 405 - Batch 227/1137 - Train loss: 0.5625267154292056, Train acc: 0.8036595394736842\n",
            "Iteration 406 - Batch 454/1137 - Train loss: 0.5589598143493736, Train acc: 0.8054258241758242\n",
            "Iteration 407 - Batch 681/1137 - Train loss: 0.5524016724041829, Train acc: 0.8077575146627566\n",
            "Iteration 408 - Batch 908/1137 - Train loss: 0.5476407486088861, Train acc: 0.8098700495049505\n",
            "Iteration 409 - Batch 1135/1137 - Train loss: 0.5485029709590993, Train acc: 0.8099416813380281\n",
            "Val loss: 0.7085789442062378, Val acc: 0.768\n",
            "Epoch 83/100\n",
            "Iteration 410 - Batch 227/1137 - Train loss: 0.5461939672629038, Train acc: 0.8099643640350878\n",
            "Iteration 411 - Batch 454/1137 - Train loss: 0.5495371186471247, Train acc: 0.8112293956043956\n",
            "Iteration 412 - Batch 681/1137 - Train loss: 0.5526612073707441, Train acc: 0.8096820014662757\n",
            "Iteration 413 - Batch 908/1137 - Train loss: 0.5492847275288061, Train acc: 0.8103169691969196\n",
            "Iteration 414 - Batch 1135/1137 - Train loss: 0.547283823929832, Train acc: 0.8106018926056338\n",
            "Val loss: 0.7148475050926208, Val acc: 0.778\n",
            "Epoch 84/100\n",
            "Iteration 415 - Batch 227/1137 - Train loss: 0.5462699776940179, Train acc: 0.8122944078947368\n",
            "Iteration 416 - Batch 454/1137 - Train loss: 0.5484166700761397, Train acc: 0.8108173076923076\n",
            "Iteration 417 - Batch 681/1137 - Train loss: 0.5518930326939678, Train acc: 0.8096361803519062\n",
            "Iteration 418 - Batch 908/1137 - Train loss: 0.5497288760801877, Train acc: 0.8098872387238724\n",
            "Iteration 419 - Batch 1135/1137 - Train loss: 0.5477956410523661, Train acc: 0.810395576584507\n",
            "Val loss: 0.6917383074760437, Val acc: 0.776\n",
            "Epoch 85/100\n",
            "Iteration 420 - Batch 227/1137 - Train loss: 0.5428246840050346, Train acc: 0.8109237938596491\n",
            "Iteration 421 - Batch 454/1137 - Train loss: 0.5405143264885787, Train acc: 0.8139423076923077\n",
            "Iteration 422 - Batch 681/1137 - Train loss: 0.5410855366599875, Train acc: 0.8119730571847508\n",
            "Iteration 423 - Batch 908/1137 - Train loss: 0.5422837583899367, Train acc: 0.8123281078107811\n",
            "Iteration 424 - Batch 1135/1137 - Train loss: 0.5432727447986393, Train acc: 0.8118672975352113\n",
            "Val loss: 0.710227370262146, Val acc: 0.786\n",
            "Epoch 86/100\n",
            "Iteration 425 - Batch 227/1137 - Train loss: 0.5423617049267417, Train acc: 0.8128426535087719\n",
            "Iteration 426 - Batch 454/1137 - Train loss: 0.5481075794487209, Train acc: 0.809271978021978\n",
            "Iteration 427 - Batch 681/1137 - Train loss: 0.5430040807842859, Train acc: 0.812133431085044\n",
            "Iteration 428 - Batch 908/1137 - Train loss: 0.5467130789006635, Train acc: 0.8111248624862486\n",
            "Iteration 429 - Batch 1135/1137 - Train loss: 0.5446882961501538, Train acc: 0.811399647887324\n",
            "Val loss: 0.646472156047821, Val acc: 0.788\n",
            "Epoch 87/100\n",
            "Iteration 430 - Batch 227/1137 - Train loss: 0.5562323546200468, Train acc: 0.8053728070175439\n",
            "Iteration 431 - Batch 454/1137 - Train loss: 0.5501847058207124, Train acc: 0.8085851648351648\n",
            "Iteration 432 - Batch 681/1137 - Train loss: 0.5474688099626223, Train acc: 0.8112399193548387\n",
            "Iteration 433 - Batch 908/1137 - Train loss: 0.5456789853286953, Train acc: 0.8127406490649065\n",
            "Iteration 434 - Batch 1135/1137 - Train loss: 0.5436857248013708, Train acc: 0.813050176056338\n",
            "Val loss: 0.6745790243148804, Val acc: 0.768\n",
            "Epoch 88/100\n",
            "Iteration 435 - Batch 227/1137 - Train loss: 0.5446860922271746, Train acc: 0.8118146929824561\n",
            "Iteration 436 - Batch 454/1137 - Train loss: 0.5417812416186699, Train acc: 0.8131868131868132\n",
            "Iteration 437 - Batch 681/1137 - Train loss: 0.5411733011346409, Train acc: 0.8139433651026393\n",
            "Iteration 438 - Batch 908/1137 - Train loss: 0.5405281631335436, Train acc: 0.8142704895489549\n",
            "Iteration 439 - Batch 1135/1137 - Train loss: 0.5405055398095242, Train acc: 0.8141642825704225\n",
            "Val loss: 0.6678476929664612, Val acc: 0.79\n",
            "Epoch 89/100\n",
            "Iteration 440 - Batch 227/1137 - Train loss: 0.5367927912688046, Train acc: 0.8158580043859649\n",
            "Iteration 441 - Batch 454/1137 - Train loss: 0.5405982937459107, Train acc: 0.8140796703296703\n",
            "Iteration 442 - Batch 681/1137 - Train loss: 0.5387604450506549, Train acc: 0.8137142595307918\n",
            "Iteration 443 - Batch 908/1137 - Train loss: 0.5396577877016089, Train acc: 0.8138063806380638\n",
            "Iteration 444 - Batch 1135/1137 - Train loss: 0.5390217629930293, Train acc: 0.8130914392605634\n",
            "Val loss: 0.7117940783500671, Val acc: 0.768\n",
            "Epoch 90/100\n",
            "Iteration 445 - Batch 227/1137 - Train loss: 0.5372140467689749, Train acc: 0.8175712719298246\n",
            "Iteration 446 - Batch 454/1137 - Train loss: 0.5365300114338215, Train acc: 0.8159684065934066\n",
            "Iteration 447 - Batch 681/1137 - Train loss: 0.5392358934949221, Train acc: 0.8144244868035191\n",
            "Iteration 448 - Batch 908/1137 - Train loss: 0.5377815028425097, Train acc: 0.8139438943894389\n",
            "Iteration 449 - Batch 1135/1137 - Train loss: 0.5387755818271511, Train acc: 0.8134352992957746\n",
            "Val loss: 0.7256614565849304, Val acc: 0.788\n",
            "Epoch 91/100\n",
            "Iteration 450 - Batch 227/1137 - Train loss: 0.5320294828791368, Train acc: 0.8140762061403509\n",
            "Iteration 451 - Batch 454/1137 - Train loss: 0.5366302790877583, Train acc: 0.812603021978022\n",
            "Iteration 452 - Batch 681/1137 - Train loss: 0.5338413287992002, Train acc: 0.8142641129032258\n",
            "Iteration 453 - Batch 908/1137 - Train loss: 0.5341257189038825, Train acc: 0.8146830308030804\n",
            "Iteration 454 - Batch 1135/1137 - Train loss: 0.5347643027633009, Train acc: 0.8140680017605634\n",
            "Val loss: 0.7326117753982544, Val acc: 0.786\n",
            "Epoch 92/100\n",
            "Iteration 455 - Batch 227/1137 - Train loss: 0.524423489706558, Train acc: 0.8191474780701754\n",
            "Iteration 456 - Batch 454/1137 - Train loss: 0.5304470913750785, Train acc: 0.8127403846153847\n",
            "Iteration 457 - Batch 681/1137 - Train loss: 0.5307861119945727, Train acc: 0.8133935117302052\n",
            "Iteration 458 - Batch 908/1137 - Train loss: 0.5318982845748087, Train acc: 0.8138751375137514\n",
            "Iteration 459 - Batch 1135/1137 - Train loss: 0.5346613874212957, Train acc: 0.8131877200704225\n",
            "Val loss: 0.7440534234046936, Val acc: 0.798\n",
            "Epoch 93/100\n",
            "Iteration 460 - Batch 227/1137 - Train loss: 0.5290133249864244, Train acc: 0.8152412280701754\n",
            "Iteration 461 - Batch 454/1137 - Train loss: 0.5341146808404189, Train acc: 0.8139079670329671\n",
            "Iteration 462 - Batch 681/1137 - Train loss: 0.5305405018791076, Train acc: 0.8156158357771262\n",
            "Iteration 463 - Batch 908/1137 - Train loss: 0.5326379432995589, Train acc: 0.8154221672167217\n",
            "Iteration 464 - Batch 1135/1137 - Train loss: 0.5342786146039274, Train acc: 0.8149757922535211\n",
            "Val loss: 0.7434386014938354, Val acc: 0.768\n",
            "Epoch 94/100\n",
            "Iteration 465 - Batch 227/1137 - Train loss: 0.5403930150103151, Train acc: 0.8093475877192983\n",
            "Iteration 466 - Batch 454/1137 - Train loss: 0.5350864327870882, Train acc: 0.8130151098901099\n",
            "Iteration 467 - Batch 681/1137 - Train loss: 0.5318365497323425, Train acc: 0.8143786656891495\n",
            "Iteration 468 - Batch 908/1137 - Train loss: 0.5335988342368564, Train acc: 0.8146486523652365\n",
            "Iteration 469 - Batch 1135/1137 - Train loss: 0.5313156983802017, Train acc: 0.8156635123239436\n",
            "Val loss: 0.756531834602356, Val acc: 0.774\n",
            "Epoch 95/100\n",
            "Iteration 470 - Batch 227/1137 - Train loss: 0.5320788692486914, Train acc: 0.8183251096491229\n",
            "Iteration 471 - Batch 454/1137 - Train loss: 0.5257127832580398, Train acc: 0.8184409340659341\n",
            "Iteration 472 - Batch 681/1137 - Train loss: 0.5259242068724898, Train acc: 0.8184109237536656\n",
            "Iteration 473 - Batch 908/1137 - Train loss: 0.5268103885244091, Train acc: 0.8179146039603961\n",
            "Iteration 474 - Batch 1135/1137 - Train loss: 0.5300943727382053, Train acc: 0.817506602112676\n",
            "Val loss: 0.7499881386756897, Val acc: 0.782\n",
            "Epoch 96/100\n",
            "Iteration 475 - Batch 227/1137 - Train loss: 0.5158304292381856, Train acc: 0.8216145833333334\n",
            "Iteration 476 - Batch 454/1137 - Train loss: 0.5252940249312055, Train acc: 0.8180975274725275\n",
            "Iteration 477 - Batch 681/1137 - Train loss: 0.5267507432580344, Train acc: 0.8177694281524927\n",
            "Iteration 478 - Batch 908/1137 - Train loss: 0.5281848021269632, Train acc: 0.8168832508250825\n",
            "Iteration 479 - Batch 1135/1137 - Train loss: 0.5292709161802917, Train acc: 0.816530039612676\n",
            "Val loss: 0.6853951215744019, Val acc: 0.796\n",
            "Epoch 97/100\n",
            "Iteration 480 - Batch 227/1137 - Train loss: 0.530843892314455, Train acc: 0.8159950657894737\n",
            "Iteration 481 - Batch 454/1137 - Train loss: 0.529192027253109, Train acc: 0.8168269230769231\n",
            "Iteration 482 - Batch 681/1137 - Train loss: 0.5320092926233395, Train acc: 0.8168530058651027\n",
            "Iteration 483 - Batch 908/1137 - Train loss: 0.5287638963645834, Train acc: 0.817433305830583\n",
            "Iteration 484 - Batch 1135/1137 - Train loss: 0.5283648157098764, Train acc: 0.8174928477112676\n",
            "Val loss: 0.6929908394813538, Val acc: 0.782\n",
            "Epoch 98/100\n",
            "Iteration 485 - Batch 227/1137 - Train loss: 0.5193290932659518, Train acc: 0.823327850877193\n",
            "Iteration 486 - Batch 454/1137 - Train loss: 0.523984678367992, Train acc: 0.8191620879120879\n",
            "Iteration 487 - Batch 681/1137 - Train loss: 0.525502780730948, Train acc: 0.8184338343108505\n",
            "Iteration 488 - Batch 908/1137 - Train loss: 0.5265939635182634, Train acc: 0.818241199119912\n",
            "Iteration 489 - Batch 1135/1137 - Train loss: 0.5273992513555666, Train acc: 0.8176441461267606\n",
            "Val loss: 0.6816037893295288, Val acc: 0.778\n",
            "Epoch 99/100\n",
            "Iteration 490 - Batch 227/1137 - Train loss: 0.5245918470777964, Train acc: 0.8153782894736842\n",
            "Iteration 491 - Batch 454/1137 - Train loss: 0.5217258369202142, Train acc: 0.8194711538461539\n",
            "Iteration 492 - Batch 681/1137 - Train loss: 0.5234828824040827, Train acc: 0.8188233137829912\n",
            "Iteration 493 - Batch 908/1137 - Train loss: 0.5260903495799328, Train acc: 0.817433305830583\n",
            "Iteration 494 - Batch 1135/1137 - Train loss: 0.5264409827593137, Train acc: 0.8177816901408451\n",
            "Val loss: 0.7252658009529114, Val acc: 0.772\n",
            "Epoch 100/100\n",
            "Iteration 495 - Batch 227/1137 - Train loss: 0.528001641364474, Train acc: 0.8162691885964912\n",
            "Iteration 496 - Batch 454/1137 - Train loss: 0.5268161161260291, Train acc: 0.8179601648351649\n",
            "Iteration 497 - Batch 681/1137 - Train loss: 0.5261963909803248, Train acc: 0.8172424853372434\n",
            "Iteration 498 - Batch 908/1137 - Train loss: 0.527079767864136, Train acc: 0.8173989273927392\n",
            "Iteration 499 - Batch 1135/1137 - Train loss: 0.5267635979624071, Train acc: 0.8172727772887324\n",
            "Val loss: 0.7222089171409607, Val acc: 0.768\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 936x360 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 ('int')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a3f81fb861a2e9177b0adba1a802b9e82311670ed2ac6abdf07a156dc26cbf26"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}